{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Bert Model for slot and intent classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/16 19:24:29 INFO mlflow.utils.credentials: Successfully connected to MLflow hosted tracking server! Host: https://636605817503717.7.gcp.databricks.com.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import TFRobertaModel\n",
    "from transformers import AutoTokenizer\n",
    "from tensorflow.keras.layers import Dropout, Dense, Flatten, Reshape, Conv1D, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, CategoricalAccuracy, BinaryAccuracy\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorflow.keras.initializers import HeNormal, GlorotUniform\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "\n",
    "loggingActive = False\n",
    "\n",
    "# connect MLFlow\n",
    "import mlflow\n",
    "if(loggingActive):\n",
    "    mlflow.login()\n",
    "\n",
    "    # set the experiment id\n",
    "    mlflow.set_experiment(experiment_id=\"939972677444421\")\n",
    "\n",
    "    mlflow.enable_system_metrics_logging()\n",
    "    mlflow.tensorflow.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "intentOutputs = []\n",
    "slotOutputs = []\n",
    "\n",
    "with open(\"../processing/JERTmate_final_data.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "    inputs = data[\"inputs\"]\n",
    "    intentOutputs = data[\"intentOutputs\"]\n",
    "    slotOutputs = data[\"slotOutputs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data - Train 80% | Validation 10% | Test 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_arrays(inputs, intentOutputs, slotOutputs, train_ratio, val_ratio, test_ratio):\n",
    "    assert len(inputs) == len(intentOutputs) == len(slotOutputs), \"All arrays must have the same length\"\n",
    "    \n",
    "    n_total = len(inputs)\n",
    "    n_train = int(n_total * train_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    \n",
    "    # split inputs\n",
    "    inputs_train, inputs_val, inputs_test = inputs[:n_train], inputs[n_train:n_train + n_val], inputs[n_train + n_val:]\n",
    "\n",
    "    # split intents\n",
    "    intentOutputs_train, intentOutputs_val, intentOutputs_test = intentOutputs[:n_train], intentOutputs[n_train:n_train + n_val], intentOutputs[n_train + n_val:]\n",
    "\n",
    "    # split slots\n",
    "    slot_type_map_train, slot_type_map_val, slot_type_map_test = [x[:60] for x in slotOutputs[:n_train]], [x[:60] for x in slotOutputs[n_train:n_train + n_val]], [x[:60] for x in slotOutputs[n_train + n_val:]]\n",
    "    slot_intent_map_train, slot_intent_map_val, slot_intent_map_test = [x[60:120] for x in slotOutputs[:n_train]], [x[60:120] for x in slotOutputs[n_train:n_train + n_val]], [x[60:120] for x in slotOutputs[n_train + n_val:]]\n",
    "    slot_action_map_train, slot_action_map_val, slot_action_map_test = [x[120:180] for x in slotOutputs[:n_train]], [x[120:180] for x in slotOutputs[n_train:n_train + n_val]], [x[120:180] for x in slotOutputs[n_train + n_val:]]\n",
    "    slot_pointers_map_train, slot_pointers_map_val, slot_pointers_map_test = [x[180:360] for x in slotOutputs[:n_train]], [x[180:360] for x in slotOutputs[n_train:n_train + n_val]], [x[180:360] for x in slotOutputs[n_train + n_val:]]\n",
    "    phantom_target_map_train, phantom_target_map_val, phantom_target_map_test = [x[360:365] for x in slotOutputs[:n_train]], [x[360:365] for x in slotOutputs[n_train:n_train + n_val]], [x[360:365] for x in slotOutputs[n_train + n_val:]]\n",
    "    phantom_intent_map_train, phantom_intent_map_val, phantom_intent_map_test = [x[365:370] for x in slotOutputs[:n_train]], [x[365:370] for x in slotOutputs[n_train:n_train + n_val]], [x[365:370] for x in slotOutputs[n_train + n_val:]]\n",
    "    phantom_action_map_train, phantom_action_map_val, phantom_action_map_test = [x[370:375] for x in slotOutputs[:n_train]], [x[370:375] for x in slotOutputs[n_train:n_train + n_val]], [x[370:375] for x in slotOutputs[n_train + n_val:]]\n",
    "    phantom_pointers_map_train, phantom_pointers_map_val, phantom_pointers_map_test = [x[375:] for x in slotOutputs[:n_train]], [x[375:] for x in slotOutputs[n_train:n_train + n_val]], [x[375:] for x in slotOutputs[n_train + n_val:]]\n",
    "\n",
    "    \n",
    "    return (tf.constant(inputs_train), tf.constant(inputs_val), tf.constant(inputs_test)), (tf.constant(intentOutputs_train), tf.constant(intentOutputs_val), tf.constant(intentOutputs_test)), (tf.constant(slot_type_map_train), tf.constant(slot_type_map_val), tf.constant(slot_type_map_test)), (tf.constant(slot_intent_map_train), tf.constant(slot_intent_map_val), tf.constant(slot_intent_map_test)), (tf.constant(slot_action_map_train), tf.constant(slot_action_map_val), tf.constant(slot_action_map_test)), (tf.constant(slot_pointers_map_train), tf.constant(slot_pointers_map_val), tf.constant(slot_pointers_map_test)), (tf.constant(phantom_target_map_train), tf.constant(phantom_target_map_val), tf.constant(phantom_target_map_test)), (tf.constant(phantom_intent_map_train), tf.constant(phantom_intent_map_val), tf.constant(phantom_intent_map_test)), (tf.constant(phantom_action_map_train), tf.constant(phantom_action_map_val), tf.constant(phantom_action_map_test)), (tf.constant(phantom_pointers_map_train), tf.constant(phantom_pointers_map_val), tf.constant(phantom_pointers_map_test))\n",
    "\n",
    "\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "(inputs_train, inputs_val, inputs_test), (intentOutputs_train, intentOutputs_val, intentOutputs_test), (slot_type_map_train, slot_type_map_val, slot_type_map_test), (slot_intent_map_train, slot_intent_map_val, slot_intent_map_test), (slot_action_map_train, slot_action_map_val, slot_action_map_test), (slot_pointers_map_train, slot_pointers_map_val, slot_pointers_map_test), (phantom_target_map_train, phantom_target_map_val, phantom_target_map_test), (phantom_intent_map_train, phantom_intent_map_val, phantom_intent_map_test), (phantom_action_map_train, phantom_action_map_val, phantom_action_map_test), (phantom_pointers_map_train, phantom_pointers_map_val, phantom_pointers_map_test) = split_arrays(inputs, intentOutputs, slotOutputs, train_ratio, val_ratio, test_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\falkt\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class JointIntentAndSlotFillingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, intent_vector_length=None, total_slot_number=None, total_phantom_slot_number=None, slot_types=None, slot_intents=None, pointer_possibilities=None, model_name=model_name, dropout_prob=0.05):\n",
    "        super().__init__(name=\"joint_intent_slot\")\n",
    "        #   ** GENERAL LAYERS **\n",
    "        self.bert = TFRobertaModel.from_pretrained(model_name) # BERT model\n",
    "        self.dropout = Dropout(dropout_prob) # basic dropout layer\n",
    "        self.flatten = Flatten() # flatten layer\n",
    "\n",
    "\n",
    "\n",
    "        # ** SLOT LAYERS **\n",
    "        # LHS compressor with HeNormal initialization\n",
    "        self.LHSC_conv1 = Conv1D(filters=256, kernel_size=1, padding='same', name='LHSC_conv1', kernel_initializer=HeNormal())\n",
    "        self.LHSC_bn1 = BatchNormalization()\n",
    "        self.LHSC_act1 = Activation('relu')\n",
    "        self.LHSC_conv2 = Conv1D(filters=64, kernel_size=1, padding='same', name='LHSC_conv2', kernel_initializer=HeNormal())\n",
    "        self.LHSC_bn2 = BatchNormalization()\n",
    "        self.LHSC_act2 = Activation('relu')\n",
    "        self.LHSC_conv3 = Conv1D(filters=32, kernel_size=1, padding='same', name='LHSC_conv3', kernel_initializer=HeNormal())\n",
    "        self.LHSC_bn3 = BatchNormalization()\n",
    "        self.LHSC_act3 = Activation('relu')\n",
    "\n",
    "        # Slot output layers with appropriate initializers\n",
    "        self.slot_type_dense = Dense(total_slot_number * slot_types, activation='softmax', name=\"slot_type_output\", kernel_initializer=GlorotUniform())\n",
    "        self.slot_type_reshape = Reshape((total_slot_number, slot_types))\n",
    "        \n",
    "        self.slot_intent_dense = Dense(total_slot_number * slot_intents, activation='softmax', name=\"slot_intent_output\", kernel_initializer=GlorotUniform())\n",
    "        self.slot_intent_reshape = Reshape((total_slot_number, slot_intents))\n",
    "        \n",
    "        self.slot_action_output = Dense(total_slot_number, activation='sigmoid', name=\"slot_action_output\", kernel_initializer=GlorotUniform())\n",
    "        \n",
    "        self.slot_pointers_dense = Dense(total_slot_number * pointer_possibilities * 3, activation='softmax', name=\"slot_pointers_output\", kernel_initializer=GlorotUniform())\n",
    "        self.slot_pointers_reshape = Reshape((total_slot_number * 3, pointer_possibilities))\n",
    "\n",
    "        # Phantom slot output layers with Glorot initialization\n",
    "        self.phantom_slot_target_dense = Dense(total_phantom_slot_number * pointer_possibilities, activation='softmax', name=\"phantom_slot_target_output\", kernel_initializer=GlorotUniform())\n",
    "        self.phantom_slot_target_reshape = Reshape((total_phantom_slot_number, pointer_possibilities))\n",
    "        \n",
    "        self.phantom_slot_intent_dense = Dense(total_phantom_slot_number * slot_intents, activation='softmax', name=\"phantom_slot_intent_output\", kernel_initializer=GlorotUniform())\n",
    "        self.phantom_slot_intent_reshape = Reshape((total_phantom_slot_number, slot_intents))\n",
    "        \n",
    "        self.phantom_slot_action_output = Dense(total_phantom_slot_number, activation='sigmoid', name=\"phantom_slot_action_output\", kernel_initializer=GlorotUniform())\n",
    "        \n",
    "        self.phantom_slot_pointers_dense = Dense(total_phantom_slot_number * pointer_possibilities * 3, activation='softmax', name=\"phantom_slot_pointers_output\", kernel_initializer=GlorotUniform())\n",
    "        self.phantom_slot_pointers_reshape = Reshape((total_phantom_slot_number * 3, pointer_possibilities))\n",
    "\n",
    "        # ** INTENT LAYERS **\n",
    "        # Processing layers with HeNormal initialization\n",
    "        self.intent_processor_one = Dense(294, name=\"intent_processor_one\", kernel_initializer=HeNormal())\n",
    "        self.intent_processor_one_bn = BatchNormalization()\n",
    "        self.intent_processor_one_act = Activation('relu')\n",
    "        self.intent_processor_two = Dense(147, name=\"intent_processor_two\", kernel_initializer=HeNormal())\n",
    "        self.intent_processor_two_bn = BatchNormalization()\n",
    "        self.intent_processor_two_act = Activation('relu')\n",
    "\n",
    "        # Output layer with Glorot initialization\n",
    "        self.intent_output = Dense(intent_vector_length, activation='softmax', name=\"intent_output\", kernel_initializer=GlorotUniform())\n",
    "\n",
    "        # Build the model with input shape (None, 1036)\n",
    "        self.build(input_shape=(None, 1036))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        bertInputs = inputs[:, :180]\n",
    "\n",
    "        # run BERT\n",
    "        trained_bert = self.bert(bertInputs, **kwargs)\n",
    "        pooled_output = trained_bert.pooler_output\n",
    "        sequence_output = trained_bert.last_hidden_state\n",
    "\n",
    "        #   ** SLOT CLASSIFICATION **\n",
    "        # use CNN to compress the sequence output\n",
    "        conv_output = self.LHSC_conv1(sequence_output)\n",
    "        conv_output = self.LHSC_bn1(conv_output, training=kwargs.get(\"training\", False))  # Apply batch normalization\n",
    "        conv_output = self.LHSC_act1(conv_output)  # Apply activation\n",
    "        conv_output = self.dropout(conv_output, training=kwargs.get(\"training\", False))  # Apply dropout\n",
    "\n",
    "        conv_output = self.LHSC_conv2(conv_output)\n",
    "        conv_output = self.LHSC_bn2(conv_output, training=kwargs.get(\"training\", False))  # Apply batch normalization\n",
    "        conv_output = self.LHSC_act2(conv_output)  # Apply activation\n",
    "        conv_output = self.dropout(conv_output, training=kwargs.get(\"training\", False))  # Apply dropout\n",
    "\n",
    "        conv_output = self.LHSC_conv3(conv_output)\n",
    "        conv_output = self.LHSC_bn3(conv_output, training=kwargs.get(\"training\", False))  # Apply batch normalization\n",
    "        conv_output = self.LHSC_act3(conv_output)  # Apply activation\n",
    "        conv_output = self.dropout(conv_output, training=kwargs.get(\"training\", False))  # Apply dropout\n",
    "\n",
    "        # flatten the compressed output\n",
    "        flattened_LHSC_output = self.flatten(conv_output)\n",
    "\n",
    "        # slot output\n",
    "        slot_output_input = self.dropout(tf.concat([flattened_LHSC_output, tf.cast(inputs[:, 180:], dtype=tf.float32)], axis=-1), training=kwargs.get(\"training\", False))\n",
    "        \n",
    "        slot_type_output = self.slot_type_dense(slot_output_input)\n",
    "        slot_type_output = self.slot_type_reshape(slot_type_output)\n",
    "        \n",
    "        slot_intent_output = self.slot_intent_dense(slot_output_input)\n",
    "        slot_intent_output = self.slot_intent_reshape(slot_intent_output)\n",
    "        \n",
    "        slot_action_output = self.slot_action_output(slot_output_input)\n",
    "        \n",
    "        slot_pointers_output = self.slot_pointers_dense(slot_output_input)\n",
    "        slot_pointers_output = self.slot_pointers_reshape(slot_pointers_output)\n",
    "\n",
    "        # Phantom slot outputs\n",
    "        phantom_target_output = self.phantom_slot_target_dense(slot_output_input)\n",
    "        phantom_target_output = self.phantom_slot_target_reshape(phantom_target_output)\n",
    "        \n",
    "        phantom_intent_output = self.phantom_slot_intent_dense(slot_output_input)\n",
    "        phantom_intent_output = self.phantom_slot_intent_reshape(phantom_intent_output)\n",
    "        \n",
    "        phantom_action_output = self.phantom_slot_action_output(slot_output_input)\n",
    "        \n",
    "        phantom_pointers_output = self.phantom_slot_pointers_dense(slot_output_input)\n",
    "        phantom_pointers_output = self.phantom_slot_pointers_reshape(phantom_pointers_output)\n",
    "\n",
    "\n",
    "\n",
    "        #   ** INTENT CLASSIFICATION **\n",
    "        # intent processor\n",
    "        intent_processor_one_input = self.dropout(tf.concat([pooled_output, tf.cast(inputs[:, 180:180 + 114], dtype=tf.float32)], axis=-1), training=kwargs.get(\"training\", False))\n",
    "        intent_processor_one = self.intent_processor_one(intent_processor_one_input)\n",
    "        intent_processor_one = self.intent_processor_one_bn(intent_processor_one, training=kwargs.get(\"training\", False))  # Apply batch normalization\n",
    "        intent_processor_one = self.intent_processor_one_act(intent_processor_one)  # Apply activation\n",
    "\n",
    "        intent_processor_two_input = self.dropout(intent_processor_one, training=kwargs.get(\"training\", False))  # Apply dropout\n",
    "        intent_processor_two = self.intent_processor_two(intent_processor_two_input)\n",
    "        intent_processor_two = self.intent_processor_two_bn(intent_processor_two, training=kwargs.get(\"training\", False))  # Apply batch normalization\n",
    "        intent_processor_two = self.intent_processor_two_act(intent_processor_two)  # Apply activation\n",
    "\n",
    "        # intent output\n",
    "        intent_output_input = self.dropout(intent_processor_two, training=kwargs.get(\"training\", False))\n",
    "        intent_output = self.intent_output(intent_output_input)\n",
    "\n",
    "        # Return outputs as a dictionary\n",
    "        return {\n",
    "            \"intent\": intent_output,\n",
    "            \"slot_type\": slot_type_output,\n",
    "            \"slot_intent\": slot_intent_output,\n",
    "            \"slot_action\": slot_action_output,\n",
    "            \"slot_pointers\": slot_pointers_output,\n",
    "            \"phantom_slot_target\": phantom_target_output,\n",
    "            \"phantom_slot_intent\": phantom_intent_output,\n",
    "            \"phantom_slot_action\": phantom_action_output,\n",
    "            \"phantom_slot_pointers\": phantom_pointers_output\n",
    "        }\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(JointIntentAndSlotFillingModel, self).get_config()\n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "joint_model = JointIntentAndSlotFillingModel(intent_vector_length=38, total_slot_number=60, total_phantom_slot_number=5, slot_types=15, slot_intents=4, pointer_possibilities=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/16 19:25:00 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "2024/10/16 19:25:00 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '0e1aa647e7064aee9688a6d96e9bcc26', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n",
      "2024/10/16 19:25:00 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: If 'features' is a TensorFlow Tensor, then 'targets' must also be a TensorFlow Tensor. Found: <class 'dict'>.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/16 19:25:03 INFO mlflow.tracking._tracking_service.client: 🏃 View run amazing-steed-605 at: https://636605817503717.7.gcp.databricks.com/ml/experiments/939972677444421/runs/0e1aa647e7064aee9688a6d96e9bcc26.\n",
      "2024/10/16 19:25:03 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://636605817503717.7.gcp.databricks.com/ml/experiments/939972677444421.\n",
      "2024/10/16 19:25:03 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/10/16 19:25:03 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 0), output.shape=(None, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m joint_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mopt, loss\u001b[38;5;241m=\u001b[39mlosses, loss_weights\u001b[38;5;241m=\u001b[39mloss_weights, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# train!\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mintent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mintentOutputs_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslot_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mslot_type_map_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslot_intent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mslot_intent_map_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslot_action\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mslot_action_map_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslot_pointers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mslot_pointers_map_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphantom_slot_target\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mphantom_target_map_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphantom_slot_intent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mphantom_intent_map_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphantom_slot_action\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mphantom_action_map_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphantom_slot_pointers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mphantom_pointers_map_train\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mintent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mintentOutputs_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslot_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mslot_type_map_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslot_intent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mslot_intent_map_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslot_action\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mslot_action_map_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslot_pointers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mslot_pointers_map_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphantom_slot_target\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mphantom_target_map_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphantom_slot_intent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mphantom_intent_map_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphantom_slot_action\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mphantom_action_map_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphantom_slot_pointers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mphantom_pointers_map_val\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:578\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    568\u001b[0m try_log_autologging_event(\n\u001b[0;32m    569\u001b[0m     AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_patch_function_start,\n\u001b[0;32m    570\u001b[0m     session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    574\u001b[0m     kwargs,\n\u001b[0;32m    575\u001b[0m )\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m patch_is_class:\n\u001b[1;32m--> 578\u001b[0m     \u001b[43mpatch_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    580\u001b[0m     patch_function(call_original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:165\u001b[0m, in \u001b[0;36mPatchFunction.call\u001b[1;34m(cls, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mcls\u001b[39m, original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:176\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[1;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_exception(e)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# Regardless of what happens during the `_on_exception` callback, reraise\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# the original implementation exception once the callback completes\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:169\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[1;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_patch_implementation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:227\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.PatchWithManagedRun._patch_implementation\u001b[1;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mactive_run():\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanaged_run \u001b[38;5;241m=\u001b[39m create_managed_run()\n\u001b[1;32m--> 227\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_patch_implementation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanaged_run:\n\u001b[0;32m    230\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mend_run(RunStatus\u001b[38;5;241m.\u001b[39mto_string(RunStatus\u001b[38;5;241m.\u001b[39mFINISHED))\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mlflow\\tensorflow\\__init__.py:1352\u001b[0m, in \u001b[0;36mautolog.<locals>.FitPatch._patch_implementation\u001b[1;34m(self, original, inst, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1346\u001b[0m         _logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1347\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to log training dataset information to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLflow Tracking. Reason: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1349\u001b[0m             e,\n\u001b[0;32m   1350\u001b[0m         )\n\u001b[1;32m-> 1352\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_models:\n\u001b[0;32m   1355\u001b[0m     _log_keras_model(history, args)\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:561\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[1;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m         original_result \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;241m*\u001b[39m_og_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_og_kwargs)\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n\u001b[1;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_original_fn_with_event_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_original_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:496\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[1;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     try_log_autologging_event(\n\u001b[0;32m    489\u001b[0m         AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_original_function_start,\n\u001b[0;32m    490\u001b[0m         session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m         og_kwargs,\n\u001b[0;32m    495\u001b[0m     )\n\u001b[1;32m--> 496\u001b[0m     original_fn_result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m     try_log_autologging_event(\n\u001b[0;32m    499\u001b[0m         AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_original_function_success,\n\u001b[0;32m    500\u001b[0m         session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m         og_kwargs,\n\u001b[0;32m    505\u001b[0m     )\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:558\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[1;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[0;32m    555\u001b[0m     disable_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    556\u001b[0m     reroute_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    557\u001b[0m ):\n\u001b[1;32m--> 558\u001b[0m     original_result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_og_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_og_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:675\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[1;34m(target, output, from_logits)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n\u001b[1;32m--> 675\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    676\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    677\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    678\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m         )\n\u001b[0;32m    681\u001b[0m output, from_logits \u001b[38;5;241m=\u001b[39m _get_logits(\n\u001b[0;32m    682\u001b[0m     output, from_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    683\u001b[0m )\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 0), output.shape=(None, 5)"
     ]
    }
   ],
   "source": [
    "# Define a learning rate schedule (e.g., cosine decay)\n",
    "total_steps = epochs * (len(inputs_train) / batch_size)\n",
    "warmup_steps = total_steps * 0.1\n",
    "lr_schedule = CosineDecay(\n",
    "    name='CosineDecay',\n",
    "\n",
    "    # main schedule parameters\n",
    "    warmup_target=1e-5, # base learning rate\n",
    "    decay_steps=total_steps - warmup_steps,\n",
    "    alpha=1e-8, # ending learning rate\n",
    "\n",
    "    # warmup parameters\n",
    "    initial_learning_rate=1e-7,\n",
    "    warmup_steps=warmup_steps\n",
    ")\n",
    "\n",
    "# optimizer\n",
    "opt = AdamW(\n",
    "    learning_rate=lr_schedule, \n",
    "    weight_decay=1e-4, \n",
    "    beta_1=0.9, \n",
    "    beta_2=0.999, \n",
    "    epsilon=1e-7, \n",
    "    clipnorm=1.0)\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=\"model_epoch_{epoch:02d}.keras\",  # Save the model with the epoch number in the filename\n",
    "    save_freq='epoch',\n",
    "    save_weights_only=False,\n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "# loss functions\n",
    "losses = {\n",
    "    \"intent\": CategoricalCrossentropy(name=\"intent_loss\"), \n",
    "    \"slot_type\": SparseCategoricalCrossentropy(from_logits=True, name=\"slot_type_loss\"),\n",
    "    \"slot_intent\": SparseCategoricalCrossentropy(from_logits=True, name=\"slot_intent_loss\"),\n",
    "    \"slot_action\": BinaryCrossentropy(name=\"slot_actionable_loss\"), \n",
    "    \"slot_pointers\": SparseCategoricalCrossentropy(from_logits=True, name=\"slot_pointer_loss\"),\n",
    "    \"phantom_slot_target\": SparseCategoricalCrossentropy(from_logits=True, name=\"phantom_slot_target_loss\"),\n",
    "    \"phantom_slot_intent\": SparseCategoricalCrossentropy(from_logits=True, name=\"phantom_slot_intent_loss\"),\n",
    "    \"phantom_slot_action\": BinaryCrossentropy(name=\"phantom_slot_actionable_loss\"), \n",
    "    \"phantom_slot_pointers\": SparseCategoricalCrossentropy(from_logits=True, name=\"phantom_slot_pointer_loss\"),\n",
    "}\n",
    "\n",
    "# loss weights\n",
    "loss_weights = {\n",
    "    \"intent\": 1.0,\n",
    "    \"slot_type\": 1.0,\n",
    "    \"slot_intent\": 1.0,\n",
    "    \"slot_action\": 0.6,\n",
    "    \"slot_pointers\": 1.0,\n",
    "    \"phantom_slot_target\": 0.8,\n",
    "    \"phantom_slot_intent\": 0.8,\n",
    "    \"phantom_slot_action\": 0.6,\n",
    "    \"phantom_slot_pointers\": 0.8,\n",
    "}\n",
    "\n",
    "# Define the metrics for each output\n",
    "metrics = {\n",
    "    \"intent\": [\n",
    "        CategoricalAccuracy(name=\"intent\"),\n",
    "    ],\n",
    "    \"slot_type\": [\n",
    "        SparseCategoricalAccuracy(name=\"slot_type\"),\n",
    "    ],\n",
    "    \"slot_intent\": [\n",
    "        SparseCategoricalAccuracy(name=\"slot_intent\"),\n",
    "    ],\n",
    "    \"slot_action\": [\n",
    "        BinaryAccuracy(name=\"slot_action\"),\n",
    "    ],\n",
    "    \"slot_pointers\": [\n",
    "        SparseCategoricalAccuracy(name=\"slot_pointer\"),\n",
    "    ],\n",
    "    \"phantom_slot_target\": [\n",
    "        SparseCategoricalAccuracy(name=\"phantom_slot_target\"),\n",
    "    ],\n",
    "    \"phantom_slot_intent\": [\n",
    "        SparseCategoricalAccuracy(name=\"phantom_slot_intent\"),\n",
    "    ],\n",
    "    \"phantom_slot_action\": [\n",
    "        BinaryAccuracy(name=\"phantom_slot_action\"),\n",
    "    ],\n",
    "    \"phantom_slot_pointers\": [\n",
    "        SparseCategoricalAccuracy(name=\"phantom_slot_pointers\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# compile model\n",
    "joint_model.compile(optimizer=opt, loss=losses, loss_weights=loss_weights, metrics=metrics)\n",
    "\n",
    "# train!\n",
    "history = joint_model.fit(\n",
    "    x=inputs_train,\n",
    "    y={\n",
    "        \"intent\": intentOutputs_train,\n",
    "        \"slot_type\": slot_type_map_train,\n",
    "        \"slot_intent\": slot_intent_map_train,\n",
    "        \"slot_action\": slot_action_map_train,\n",
    "        \"slot_pointers\": slot_pointers_map_train,\n",
    "        \"phantom_slot_target\": phantom_target_map_train,\n",
    "        \"phantom_slot_intent\": phantom_intent_map_train,\n",
    "        \"phantom_slot_action\": phantom_action_map_train,\n",
    "        \"phantom_slot_pointers\": phantom_pointers_map_train\n",
    "    }, \n",
    "    validation_data=(\n",
    "        inputs_val,\n",
    "        {\n",
    "            \"intent\": intentOutputs_val,\n",
    "            \"slot_type\": slot_type_map_val,\n",
    "            \"slot_intent\": slot_intent_map_val,\n",
    "            \"slot_action\": slot_action_map_val,\n",
    "            \"slot_pointers\": slot_pointers_map_val,\n",
    "            \"phantom_slot_target\": phantom_target_map_val,\n",
    "            \"phantom_slot_intent\": phantom_intent_map_val,\n",
    "            \"phantom_slot_action\": phantom_action_map_val,\n",
    "            \"phantom_slot_pointers\": phantom_pointers_map_val\n",
    "        }\n",
    "    ),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "\n",
    "keep in mind -> test slot accuracy will be higher than reality.\n",
    "        Because 90% of the data points are 0, it can just guess 0 and be right 85% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_intent_acc, test_slot_type, test_slot_intent, test_slot_action, test_slot_pointers, test_phantom_target, test_phantom_intent, test_phantom_action, test_phantom_pointers = joint_model.evaluate(x=inputs_test, y=(intentOutputs_test, slot_type_map_test, slot_intent_map_test, slot_action_map_test, slot_pointers_map_test, phantom_target_map_test, phantom_intent_map_test, phantom_action_map_test, phantom_pointers_map_test), batch_size=batch_size)\n",
    "\n",
    "print(f\"Test Intent Accuracy: {test_intent_acc}\")\n",
    "print(f\"Test Slot Type Accuracy: {test_slot_type}\")\n",
    "print(f\"Test Slot Intent Accuracy: {test_slot_intent}\")\n",
    "print(f\"Test Slot Action Accuracy: {test_slot_action}\")\n",
    "print(f\"Test Slot Pointers Accuracy: {test_slot_pointers}\")\n",
    "print(f\"Test Phantom Target Accuracy: {test_phantom_target}\")\n",
    "print(f\"Test Phantom Intent Accuracy: {test_phantom_intent}\")\n",
    "print(f\"Test Phantom Action Accuracy: {test_phantom_action}\")\n",
    "print(f\"Test Phantom Pointers Accuracy: {test_phantom_pointers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def inferConversation(conversation):\n",
    "    conversation = conversation.split(\"|\")\n",
    "    memory = []\n",
    "    for i in range(len(conversation)):\n",
    "        textInput = ''\n",
    "        for i in range(max(i - 2, 0), i):\n",
    "            textInput += conversation[i] + ' [SEP] '\n",
    "        textInput += conversation[i]\n",
    "    \n",
    "        output = inferSentence(textInput, memory)\n",
    "\n",
    "        # update memory\n",
    "        if len(memory) > 2:\n",
    "            memory.pop(0)    \n",
    "        memory.append(output)\n",
    "\n",
    "def inferSentence(sentence, memory):\n",
    "    # tokenize\n",
    "    input = tokenizer(sentence, return_tensors=\"tf\", padding=\"max_length\", max_length=150, truncation=True)\n",
    "\n",
    "    # compile memory\n",
    "    intentMemory = []\n",
    "    slotMemory = []\n",
    "    for key in input:\n",
    "        intentMemory.extend(key[\"intent_output\"])\n",
    "        slotMemory.extend([key[\"slot_type_output\"], key[\"slot_intent_output\"], key[\"slot_action_output\"], key[\"slot_pointers_output\"], key[\"phantom_slot_target_output\"], key[\"phantom_slot_intent_output\"], key[\"phantom_slot_action_output\"], key[\"phantom_slot_pointers_output\"]])\n",
    "\n",
    "    input.extend(intentMemory)\n",
    "    input.extend(slotMemory)\n",
    "\n",
    "    input = memory.extend(input)\n",
    "\n",
    "    # predict\n",
    "    output = joint_model.predict(input)\n",
    "\n",
    "    return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
