{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Bert Model for slot and intent classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, CategoricalAccuracy\n",
    "\n",
    "model_name = \"distilbert/distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "intentOutputs = []\n",
    "slotOutputs = []\n",
    "\n",
    "with open(\"../processing/JERTmate_final_data.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "    inputs = data[\"inputs\"]\n",
    "    intentOutputs = data[\"intentOutputs\"]\n",
    "    slotOutputs = data[\"slotOutputs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data - Train 80% | Validation 10% | Test 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_arrays(inputs, intentOutputs, slotOutputs, train_ratio, val_ratio, test_ratio, seed=42):\n",
    "    assert len(inputs) == len(intentOutputs) == len(slotOutputs), \"All arrays must have the same length\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(inputs))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    n_total = len(inputs)\n",
    "    n_train = int(n_total * train_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    \n",
    "    inputs_train, inputs_val, inputs_test = inputs[:n_train], inputs[n_train:n_train + n_val], inputs[n_train + n_val:]\n",
    "    intentOutputs_train, intentOutputs_val, intentOutputs_test = intentOutputs[:n_train], intentOutputs[n_train:n_train + n_val], intentOutputs[n_train + n_val:]\n",
    "    slotOutputs_train, slotOutputs_val, slotOutputs_test = slotOutputs[:n_train], slotOutputs[n_train:n_train + n_val], slotOutputs[n_train + n_val:]\n",
    "    \n",
    "    return (inputs_train, inputs_val, inputs_test), (intentOutputs_train, intentOutputs_val, intentOutputs_test), (slotOutputs_train, slotOutputs_val, slotOutputs_test)\n",
    "\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "(inputs_train, inputs_val, inputs_test), (intentOutputs_train, intentOutputs_val, intentOutputs_test), (slotOutputs_train, slotOutputs_val, slotOutputs_test) = split_arrays(inputs, intentOutputs, slotOutputs, train_ratio, val_ratio, test_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "class JointIntentAndSlotFillingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, intent_num_labels=None, slot_num_labels=None, model_name=model_name, dropout_prob=0.1):\n",
    "        super().__init__(name=\"joint_intent_slot\")\n",
    "        self.bert = TFBertModel.from_pretrained(model_name)\n",
    "        self.dropout = Dropout(dropout_prob)\n",
    "        self.intent_classifier = Dense(intent_num_labels, name=\"intent_classifier\")\n",
    "        self.slot_classifier = Dense(slot_num_labels, name=\"slot_classifier\")\n",
    "\n",
    "    def __call__(self, inputs, **kwargs):\n",
    "        # two outputs from BERT\n",
    "        trained_bert = self.bert(inputs[0], **kwargs)\n",
    "        pooled_output = trained_bert.pooler_output\n",
    "        sequence_output = trained_bert.last_hidden_state\n",
    "\n",
    "        # slot_filling / classification\n",
    "        slot_input = tf.concat([sequence_output, tf.reshape(inputs[1:], [-1])], axis=-1) # Combine sequence output and all memory data\n",
    "\n",
    "        slot_input = self.dropout(slot_input, training=kwargs.get(\"training\", False))\n",
    "        slot_logits = self.slot_classifier(slot_input)\n",
    "\n",
    "        # intent classification\n",
    "        intent_input = tf.concat([pooled_output, inputs[1]], axis=-1) # Combine pooled output and memory intent data\n",
    "\n",
    "        intent_input = self.dropout(intent_input, training=kwargs.get(\"training\", False))\n",
    "        intent_logits = self.intent_classifier(intent_input)\n",
    "\n",
    "        return slot_logits, intent_logits, sequence_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(JointIntentAndSlotFillingModel, self).get_config()\n",
    "        config.update({\n",
    "            \"dropout\": self.dropout_prob,\n",
    "            \"intent_num_labels\": self.intent_num_labels,\n",
    "            \"slot_num_labels\": self.slot_num_labels,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "joint_model = JointIntentAndSlotFillingModel(intent_num_labels=38, slot_num_labels=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.3142 - loss: 2.5271 - slot_accuracy: 0.9288\n",
      "Epoch 1: saving model to model_epoch_01.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7231s\u001b[0m 3s/step - intent_accuracy: 0.3142 - loss: 2.5270 - slot_accuracy: 0.9288 - val_intent_accuracy: 0.4787 - val_loss: 1.7735 - val_slot_accuracy: 0.9645 - learning_rate: 5.0000e-05\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:102: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\n",
      "  return saving_lib.save_model(model, filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5354 - loss: 1.6793 - slot_accuracy: 0.9663\n",
      "Epoch 2: saving model to model_epoch_02.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7147s\u001b[0m 3s/step - intent_accuracy: 0.5354 - loss: 1.6793 - slot_accuracy: 0.9663 - val_intent_accuracy: 0.6088 - val_loss: 1.4713 - val_slot_accuracy: 0.9712 - learning_rate: 5.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.6301 - loss: 1.4190 - slot_accuracy: 0.9724\n",
      "Epoch 3: saving model to model_epoch_03.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7028s\u001b[0m 3s/step - intent_accuracy: 0.6301 - loss: 1.4190 - slot_accuracy: 0.9724 - val_intent_accuracy: 0.6665 - val_loss: 1.2910 - val_slot_accuracy: 0.9754 - learning_rate: 5.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.6813 - loss: 1.2516 - slot_accuracy: 0.9760\n",
      "Epoch 4: saving model to model_epoch_04.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7001s\u001b[0m 3s/step - intent_accuracy: 0.6813 - loss: 1.2516 - slot_accuracy: 0.9760 - val_intent_accuracy: 0.7023 - val_loss: 1.1652 - val_slot_accuracy: 0.9777 - learning_rate: 5.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.7164 - loss: 1.1361 - slot_accuracy: 0.9778\n",
      "Epoch 5: saving model to model_epoch_05.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7024s\u001b[0m 3s/step - intent_accuracy: 0.7164 - loss: 1.1361 - slot_accuracy: 0.9778 - val_intent_accuracy: 0.7357 - val_loss: 1.0699 - val_slot_accuracy: 0.9792 - learning_rate: 5.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.7442 - loss: 1.0372 - slot_accuracy: 0.9793\n",
      "Epoch 6: saving model to model_epoch_06.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7008s\u001b[0m 3s/step - intent_accuracy: 0.7442 - loss: 1.0372 - slot_accuracy: 0.9793 - val_intent_accuracy: 0.7607 - val_loss: 0.9952 - val_slot_accuracy: 0.9801 - learning_rate: 5.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.7647 - loss: 0.9673 - slot_accuracy: 0.9802\n",
      "Epoch 7: saving model to model_epoch_07.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6988s\u001b[0m 3s/step - intent_accuracy: 0.7647 - loss: 0.9673 - slot_accuracy: 0.9802 - val_intent_accuracy: 0.7680 - val_loss: 0.9317 - val_slot_accuracy: 0.9808 - learning_rate: 5.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.7801 - loss: 0.9113 - slot_accuracy: 0.9808\n",
      "Epoch 8: saving model to model_epoch_08.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6975s\u001b[0m 3s/step - intent_accuracy: 0.7801 - loss: 0.9113 - slot_accuracy: 0.9808 - val_intent_accuracy: 0.7874 - val_loss: 0.8793 - val_slot_accuracy: 0.9813 - learning_rate: 5.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.7980 - loss: 0.8553 - slot_accuracy: 0.9812\n",
      "Epoch 9: saving model to model_epoch_09.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6941s\u001b[0m 3s/step - intent_accuracy: 0.7980 - loss: 0.8553 - slot_accuracy: 0.9812 - val_intent_accuracy: 0.8003 - val_loss: 0.8343 - val_slot_accuracy: 0.9817 - learning_rate: 5.0000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8104 - loss: 0.8085 - slot_accuracy: 0.9817\n",
      "Epoch 10: saving model to model_epoch_10.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7056s\u001b[0m 3s/step - intent_accuracy: 0.8104 - loss: 0.8085 - slot_accuracy: 0.9817 - val_intent_accuracy: 0.8087 - val_loss: 0.7938 - val_slot_accuracy: 0.9821 - learning_rate: 5.0000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8221 - loss: 0.7699 - slot_accuracy: 0.9820\n",
      "Epoch 11: saving model to model_epoch_11.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7133s\u001b[0m 3s/step - intent_accuracy: 0.8221 - loss: 0.7699 - slot_accuracy: 0.9820 - val_intent_accuracy: 0.8213 - val_loss: 0.7583 - val_slot_accuracy: 0.9824 - learning_rate: 5.0000e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8323 - loss: 0.7345 - slot_accuracy: 0.9823\n",
      "Epoch 12: saving model to model_epoch_12.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7215s\u001b[0m 3s/step - intent_accuracy: 0.8323 - loss: 0.7345 - slot_accuracy: 0.9823 - val_intent_accuracy: 0.8298 - val_loss: 0.7281 - val_slot_accuracy: 0.9826 - learning_rate: 5.0000e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8409 - loss: 0.7015 - slot_accuracy: 0.9826\n",
      "Epoch 13: saving model to model_epoch_13.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7174s\u001b[0m 3s/step - intent_accuracy: 0.8409 - loss: 0.7015 - slot_accuracy: 0.9826 - val_intent_accuracy: 0.8358 - val_loss: 0.6997 - val_slot_accuracy: 0.9829 - learning_rate: 5.0000e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8457 - loss: 0.6803 - slot_accuracy: 0.9827\n",
      "Epoch 14: saving model to model_epoch_14.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7134s\u001b[0m 3s/step - intent_accuracy: 0.8457 - loss: 0.6803 - slot_accuracy: 0.9827 - val_intent_accuracy: 0.8436 - val_loss: 0.6764 - val_slot_accuracy: 0.9830 - learning_rate: 4.6000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8505 - loss: 0.6594 - slot_accuracy: 0.9828\n",
      "Epoch 15: saving model to model_epoch_15.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7108s\u001b[0m 3s/step - intent_accuracy: 0.8505 - loss: 0.6594 - slot_accuracy: 0.9828 - val_intent_accuracy: 0.8465 - val_loss: 0.6569 - val_slot_accuracy: 0.9832 - learning_rate: 4.2320e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8544 - loss: 0.6421 - slot_accuracy: 0.9830\n",
      "Epoch 16: saving model to model_epoch_16.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7128s\u001b[0m 3s/step - intent_accuracy: 0.8544 - loss: 0.6421 - slot_accuracy: 0.9830 - val_intent_accuracy: 0.8520 - val_loss: 0.6392 - val_slot_accuracy: 0.9832 - learning_rate: 3.8934e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8614 - loss: 0.6206 - slot_accuracy: 0.9832\n",
      "Epoch 17: saving model to model_epoch_17.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7137s\u001b[0m 3s/step - intent_accuracy: 0.8614 - loss: 0.6206 - slot_accuracy: 0.9832 - val_intent_accuracy: 0.8579 - val_loss: 0.6244 - val_slot_accuracy: 0.9834 - learning_rate: 3.5820e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8651 - loss: 0.6069 - slot_accuracy: 0.9832\n",
      "Epoch 18: saving model to model_epoch_18.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7168s\u001b[0m 3s/step - intent_accuracy: 0.8651 - loss: 0.6069 - slot_accuracy: 0.9832 - val_intent_accuracy: 0.8594 - val_loss: 0.6118 - val_slot_accuracy: 0.9835 - learning_rate: 3.2954e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m2320/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17:33\u001b[0m 2s/step - intent_accuracy: 0.8682 - loss: 0.5948 - slot_accuracy: 0.9833"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "opt = Adam(learning_rate=5e-5, epsilon=1e-08)\n",
    "\n",
    "# learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 7:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.97\n",
    "learning_rate_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=\"model_epoch_{epoch:02d}.keras\",  # Save the model with the epoch number in the filename\n",
    "    save_freq='epoch',\n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "# two losses, one for slots, another for intents\n",
    "losses = [SparseCategoricalCrossentropy(from_logits=True, name=\"slot_loss\"), CategoricalCrossentropy(from_logits=True, name=\"intent_loss\")]\n",
    "metrics = [SparseCategoricalAccuracy(name=\"slot_accuracy\"), CategoricalAccuracy(name=\"intent_accuracy\")]\n",
    "\n",
    "# compile model\n",
    "joint_model.compile(optimizer=opt, loss=losses, metrics=metrics)\n",
    "\n",
    "history = joint_model.fit(\n",
    "    x=inputs_train, \n",
    "    y=(slotOutputs_train, intentOutputs_train), \n",
    "    validation_data=(inputs_val, (slotOutputs_val, intentOutputs_val)), \n",
    "    epochs=10, \n",
    "    batch_size=16,\n",
    "    shuffle=True, \n",
    "    callbacks=[learning_rate_callback, checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "\n",
    "keep in mind -> test slot accuracy will be higher than reality.\n",
    "        Because 90% of the data points are 0, it can just guess 0 and be right 85% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must call `compile()` before using the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_loss, test_slot_acc, test_intent_acc \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_encoded_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_encoded_slots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_encoded_intents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Slot Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_slot_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Intent Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_intent_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py:993\u001b[0m, in \u001b[0;36mTrainer._assert_compile_called\u001b[1;34m(self, method_name)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    992\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: You must call `compile()` before using the model."
     ]
    }
   ],
   "source": [
    "test_loss, test_slot_acc, test_intent_acc = joint_model.evaluate(x=inputs_test, y=(slotOutputs_test, intentOutputs_test), batch_size=32)\n",
    "\n",
    "print(f\"Test Slot Accuracy: {test_slot_acc}\")\n",
    "print(f\"Test Intent Accuracy: {test_intent_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlu(text, tokenizer, model, intent_names, slot_names):\n",
    "    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n",
    "    outputs = model(inputs)\n",
    "    slot_logits, intent_logits = outputs\n",
    "\n",
    "    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, :]\n",
    "    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n",
    "\n",
    "    info = {\"intent\": intent_names[intent_id], \"slots\": {}}\n",
    "\n",
    "    out_dict = {}\n",
    "    # get all slot names and add to out_dict as keys\n",
    "    predicted_slots = set([re.sub(r'^[BI]-', '', slot_names[s]) for s in slot_ids if s != 0])\n",
    "    for ps in predicted_slots:\n",
    "      out_dict[ps] = []\n",
    "\n",
    "    # check if the text starts with a small letter\n",
    "    if text[0].islower():\n",
    "      tokens = tokenizer.tokenize(text, add_special_tokens=True)\n",
    "    else:\n",
    "      tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # process sequence output for slots\n",
    "    for token, slot_id in zip(tokens, slot_ids):\n",
    "      # add all to out_dict\n",
    "      slot_name = slot_names[slot_id]\n",
    "\n",
    "      if slot_name == \"[PAD]\":\n",
    "        continue\n",
    "\n",
    "      # collect tokens\n",
    "      collected_tokens = [token]\n",
    "      idx = tokens.index(token)\n",
    "\n",
    "      # see if it starts with ##\n",
    "      # then it belongs to the previous token\n",
    "      if token.startswith(\"##\"):\n",
    "\n",
    "        # check if the token already exists or not\n",
    "        if tokens[idx - 1] not in out_dict[slot_name]:\n",
    "          collected_tokens.insert(0, tokens[idx - 1])\n",
    "\n",
    "      # add collected tokens to slots\n",
    "      out_dict[slot_name].extend(collected_tokens)\n",
    "\n",
    "    # process out_dict\n",
    "    for slot_name in out_dict:\n",
    "      tokens = out_dict[slot_name]\n",
    "      slot_value = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "      info[\"slots\"][slot_name] = slot_value.strip()\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': 'add_res', 'slots': {'NAME': 'thirty', 'ITEM': 'could you please reservation december originally scheduled twelve thirty in morning at four five', 'DRINK': '[CLS] change twenty', 'DATE': 'my on for the now be', 'SIZE': 'first to pm', 'NUMBER': '[SEP]'}}\n"
     ]
    }
   ],
   "source": [
    "#loaded_model = tf.keras.models.load_model('model_epoch_18.keras', custom_objects={'JointIntentAndSlotFillingModel': JointIntentAndSlotFillingModel})\n",
    "\n",
    "print(nlu(\"could you please change my reservation on december twenty first originally scheduled for twelve thirty in the morning to now be at four thirty five pm\", tokenizer, joint_model, intent_names, slot_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
