{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Bert Model for slot and intent classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/05 17:58:45 INFO mlflow.utils.credentials: Successfully connected to MLflow hosted tracking server! Host: https://community.cloud.databricks.com.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Dropout, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, CategoricalAccuracy, BinaryAccuracy\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# connect MLFlow\n",
    "import mlflow\n",
    "mlflow.login()\n",
    "\n",
    "# set the experiment id\n",
    "mlflow.set_experiment(experiment_id=\"1400101170489055\")\n",
    "\n",
    "mlflow.tensorflow.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "intentOutputs = []\n",
    "slotOutputs = []\n",
    "\n",
    "with open(\"../processing/JERTmate_final_data.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "    inputs = data[\"inputs\"]\n",
    "    intentOutputs = data[\"intentOutputs\"]\n",
    "    slotOutputs = data[\"slotOutputs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data - Train 80% | Validation 10% | Test 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34682\n",
      "(20809, 38)\n"
     ]
    }
   ],
   "source": [
    "def split_arrays(inputs, intentOutputs, slotOutputs, train_ratio, val_ratio, test_ratio):\n",
    "    assert len(inputs) == len(intentOutputs) == len(slotOutputs), \"All arrays must have the same length\"\n",
    "    \n",
    "    n_total = len(inputs)\n",
    "    n_train = int(n_total * train_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    \n",
    "    # split inputs\n",
    "    inputs_train, inputs_val, inputs_test = inputs[:n_train], inputs[n_train:n_train + n_val], inputs[n_train + n_val:]\n",
    "\n",
    "    # split intents\n",
    "    intentOutputs_train, intentOutputs_val, intentOutputs_test = intentOutputs[:n_train], intentOutputs[n_train:n_train + n_val], intentOutputs[n_train + n_val:]\n",
    "\n",
    "    # split slots\n",
    "    slot_type_map_train, slot_type_map_val, slot_type_map_test = [x[:50] for x in slotOutputs[:n_train]], [x[:50] for x in slotOutputs[n_train:n_train + n_val]], [x[:50] for x in slotOutputs[n_train + n_val:]]\n",
    "    slot_intent_map_train, slot_intent_map_val, slot_intent_map_test = [x[50:100] for x in slotOutputs[:n_train]], [x[50:100] for x in slotOutputs[n_train:n_train + n_val]], [x[50:100] for x in slotOutputs[n_train + n_val:]]\n",
    "    slot_action_map_train, slot_action_map_val, slot_action_map_test = [x[100:150] for x in slotOutputs[:n_train]], [x[100:150] for x in slotOutputs[n_train:n_train + n_val]], [x[100:150] for x in slotOutputs[n_train + n_val:]]\n",
    "    slot_pointers_map_train, slot_pointers_map_val, slot_pointers_map_test = [x[150:300] for x in slotOutputs[:n_train]], [x[150:300] for x in slotOutputs[n_train:n_train + n_val]], [x[150:300] for x in slotOutputs[n_train + n_val:]]\n",
    "    phantom_target_map_train, phantom_target_map_val, phantom_target_map_test = [x[300:305] for x in slotOutputs[:n_train]], [x[300:305] for x in slotOutputs[n_train:n_train + n_val]], [x[300:305] for x in slotOutputs[n_train + n_val:]]\n",
    "    phantom_intent_map_train, phantom_intent_map_val, phantom_intent_map_test = [x[305:310] for x in slotOutputs[:n_train]], [x[305:310] for x in slotOutputs[n_train:n_train + n_val]], [x[305:310] for x in slotOutputs[n_train + n_val:]]\n",
    "    phantom_action_map_train, phantom_action_map_val, phantom_action_map_test = [x[310:315] for x in slotOutputs[:n_train]], [x[310:315] for x in slotOutputs[n_train:n_train + n_val]], [x[310:315] for x in slotOutputs[n_train + n_val:]]\n",
    "    phantom_pointers_map_train, phantom_pointers_map_val, phantom_pointers_map_test = [x[315:] for x in slotOutputs[:n_train]], [x[315:] for x in slotOutputs[n_train:n_train + n_val]], [x[315:] for x in slotOutputs[n_train + n_val:]]\n",
    "\n",
    "    \n",
    "    return (tf.constant(inputs_train), tf.constant(inputs_val), tf.constant(inputs_test)), (tf.constant(intentOutputs_train), tf.constant(intentOutputs_val), tf.constant(intentOutputs_test)), (tf.constant(slot_type_map_train), tf.constant(slot_type_map_val), tf.constant(slot_type_map_test)), (tf.constant(slot_intent_map_train), tf.constant(slot_intent_map_val), tf.constant(slot_intent_map_test)), (tf.constant(slot_action_map_train), tf.constant(slot_action_map_val), tf.constant(slot_action_map_test)), (tf.constant(slot_pointers_map_train), tf.constant(slot_pointers_map_val), tf.constant(slot_pointers_map_test)), (tf.constant(phantom_target_map_train), tf.constant(phantom_target_map_val), tf.constant(phantom_target_map_test)), (tf.constant(phantom_intent_map_train), tf.constant(phantom_intent_map_val), tf.constant(phantom_intent_map_test)), (tf.constant(phantom_action_map_train), tf.constant(phantom_action_map_val), tf.constant(phantom_action_map_test)), (tf.constant(phantom_pointers_map_train), tf.constant(phantom_pointers_map_val), tf.constant(phantom_pointers_map_test))\n",
    "\n",
    "\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "print(len(inputs))\n",
    "\n",
    "(inputs_train, inputs_val, inputs_test), (intentOutputs_train, intentOutputs_val, intentOutputs_test), (slot_type_map_train, slot_type_map_val, slot_type_map_test), (slot_intent_map_train, slot_intent_map_val, slot_intent_map_test), (slot_action_map_train, slot_action_map_val, slot_action_map_test), (slot_pointers_map_train, slot_pointers_map_val, slot_pointers_map_test), (phantom_target_map_train, phantom_target_map_val, phantom_target_map_test), (phantom_intent_map_train, phantom_intent_map_val, phantom_intent_map_test), (phantom_action_map_train, phantom_action_map_val, phantom_action_map_test), (phantom_pointers_map_train, phantom_pointers_map_val, phantom_pointers_map_test) = split_arrays(inputs, intentOutputs, slotOutputs, train_ratio, val_ratio, test_ratio)\n",
    "\n",
    "print(intentOutputs_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "class JointIntentAndSlotFillingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, intent_vector_length=None, total_slot_number=None, total_phantom_slot_number=None, slot_types=None, slot_intents=None, pointer_possibilities=None, model_name=model_name, dropout_prob=0.08):\n",
    "        super().__init__(name=\"joint_intent_slot\")\n",
    "        #   ** GENERAL LAYERS **\n",
    "        self.bert = TFBertModel.from_pretrained(model_name) # BERT model\n",
    "        self.dropout = Dropout(dropout_prob) # basic dropout layer\n",
    "        self.flatten = Flatten() # flatten layer\n",
    "\n",
    "\n",
    "\n",
    "        #   ** SLOT LAYERS **\n",
    "        # LHS compressor\n",
    "        #?self.conv_layer = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 768), strides=(1, 1),padding='valid',activation='relu')\n",
    "        self.last_hidden_sequence_compressor = Dense(450, activation=\"relu\", name=\"last_hidden_sequence_compressor\")\n",
    "\n",
    "        # processing layers\n",
    "        #?self.slot_processor = Dense(((150) + (intent_vector_length * 2) + (slot_vector_length * 2)), activation=\"relu\", name=\"slot_processor\")\n",
    "\n",
    "        # slot output layers\n",
    "        self.slot_type_dense = Dense(total_slot_number * slot_types, activation='softmax', name=\"slot_type_output\")\n",
    "        self.slot_type_reshape = Reshape((total_slot_number, slot_types))\n",
    "        \n",
    "        self.slot_intent_dense = Dense(total_slot_number * slot_intents, activation='softmax', name=\"slot_intent_output\")\n",
    "        self.slot_intent_reshape = Reshape((total_slot_number, slot_intents))\n",
    "        \n",
    "        self.slot_action_output = Dense(total_slot_number, activation='softmax', name=\"slot_action_output\")\n",
    "        \n",
    "        self.slot_pointers_dense = Dense(total_slot_number * pointer_possibilities * 3, activation='softmax', name=\"slot_pointers_output\")\n",
    "        self.slot_pointers_reshape = Reshape((total_slot_number * 3, pointer_possibilities))\n",
    "\n",
    "        # Phantom slot output layers\n",
    "        self.phantom_slot_target_dense = Dense(total_phantom_slot_number * pointer_possibilities, activation='softmax', name=\"phantom_slot_target_output\")\n",
    "        self.phantom_slot_target_reshape = Reshape((total_phantom_slot_number, pointer_possibilities))\n",
    "        \n",
    "        self.phantom_slot_intent_dense = Dense(total_phantom_slot_number * slot_intents, activation='softmax', name=\"phantom_slot_intent_output\")\n",
    "        self.phantom_slot_intent_reshape = Reshape((total_phantom_slot_number, slot_intents))\n",
    "        \n",
    "        self.phantom_slot_action_output = Dense(total_phantom_slot_number, activation='softmax', name=\"phantom_slot_action_output\")\n",
    "        \n",
    "        self.phantom_slot_pointers_dense = Dense(total_phantom_slot_number * pointer_possibilities * 3, activation='softmax', name=\"phantom_slot_pointers_output\")\n",
    "        self.phantom_slot_pointers_reshape = Reshape((total_phantom_slot_number * 3, pointer_possibilities))\n",
    "\n",
    "\n",
    "\n",
    "        #  ** INTENT LAYERS **\n",
    "        # processing layers\n",
    "        #?self.intent_processor = Dense(((150 * 1) + (intent_vector_length * 2)), activation=\"relu\", name=\"intent_processor\")\n",
    "\n",
    "        # output layer\n",
    "        self.intent_output = Dense(intent_vector_length, activation='softmax', name=\"intent_output\")\n",
    "\n",
    "    def __call__(self, inputs, **kwargs):\n",
    "        # run BERT\n",
    "        trained_bert = self.bert(inputs[:, :150], **kwargs)\n",
    "        pooled_output = trained_bert.pooler_output\n",
    "        sequence_output = trained_bert.last_hidden_state\n",
    "\n",
    "        #   ** SLOT CLASSIFICATION **\n",
    "        # reduce dimensionality of sequence output - Dense(150)\n",
    "        flattened_sequence_output = self.flatten(sequence_output)\n",
    "        LHSC_output = self.last_hidden_sequence_compressor(flattened_sequence_output)\n",
    "\n",
    "        # slot processor\n",
    "        #slot_processor_input = self.dropout(tf.concat([LHSC_output, tf.cast(inputs[:, 150:], dtype=tf.float32)], axis=-1), training=kwargs.get(\"training\", False))\n",
    "        #slot_processor_output = self.slot_processor(slot_processor_input)\n",
    "\n",
    "        # slot output\n",
    "        slot_output_input = self.dropout(tf.concat([LHSC_output, tf.cast(inputs[:, 150:], dtype=tf.float32)], axis=-1), training=kwargs.get(\"training\", False))\n",
    "        \n",
    "        slot_type_output = self.slot_type_dense(slot_output_input)\n",
    "        slot_type_output = self.slot_type_reshape(slot_type_output)\n",
    "        \n",
    "        slot_intent_output = self.slot_intent_dense(slot_output_input)\n",
    "        slot_intent_output = self.slot_intent_reshape(slot_intent_output)\n",
    "        \n",
    "        slot_action_output = self.slot_action_output(slot_output_input)\n",
    "        \n",
    "        slot_pointers_output = self.slot_pointers_dense(slot_output_input)\n",
    "        slot_pointers_output = self.slot_pointers_reshape(slot_pointers_output)\n",
    "\n",
    "        # Phantom slot outputs\n",
    "        phantom_target_output = self.phantom_slot_target_dense(slot_output_input)\n",
    "        phantom_target_output = self.phantom_slot_target_reshape(phantom_target_output)\n",
    "        \n",
    "        phantom_intent_output = self.phantom_slot_intent_dense(slot_output_input)\n",
    "        phantom_intent_output = self.phantom_slot_intent_reshape(phantom_intent_output)\n",
    "        \n",
    "        phantom_action_output = self.phantom_slot_action_output(slot_output_input)\n",
    "        \n",
    "        phantom_pointers_output = self.phantom_slot_pointers_dense(slot_output_input)\n",
    "        phantom_pointers_output = self.phantom_slot_pointers_reshape(phantom_pointers_output)\n",
    "\n",
    "\n",
    "\n",
    "        #   ** INTENT CLASSIFICATION **\n",
    "        # intent processor\n",
    "        #intent_processor_input = self.dropout(tf.concat([pooled_output, tf.cast(inputs[:, 150:150 + 114], dtype=tf.float32)], axis=-1), training=kwargs.get(\"training\", False))\n",
    "        #intent_processor_output = self.intent_processor(intent_processor_input)\n",
    "\n",
    "        # intent output\n",
    "        intent_output_input = self.dropout(tf.concat([pooled_output, tf.cast(inputs[:, 150:150 + 114], dtype=tf.float32)], axis=-1), training=kwargs.get(\"training\", False))\n",
    "        intent_output = self.intent_output(intent_output_input)\n",
    "\n",
    "        return intent_output, slot_type_output, slot_intent_output, slot_action_output, slot_pointers_output, phantom_target_output, phantom_intent_output, phantom_action_output, phantom_pointers_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(JointIntentAndSlotFillingModel, self).get_config()\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "joint_model = JointIntentAndSlotFillingModel(intent_vector_length=38, total_slot_number=50, total_phantom_slot_number=5, slot_types=15, slot_intents=4, pointer_possibilities=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/05 17:59:00 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'e0a1be0dafc047ea961a0d7762da2711', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n",
      "2024/10/05 17:59:00 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: 'JointIntentAndSlotFillingModel' object has no attribute 'input_shape'\n",
      "2024/10/05 17:59:00 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: If 'features' is a TensorFlow Tensor, then 'targets' must also be a TensorFlow Tensor. Found: <class 'tuple'>.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.2508 - loss: 23.8816 - phantom_slot_actionable_accuracy: 0.2112 - phantom_slot_intent_accuracy: 0.6448 - phantom_slot_pointer_accuracy: 0.5454 - phantom_slot_target_accuracy: 0.1591 - slot_actionable_accuracy: 0.0115 - slot_intent_accuracy: 0.7782 - slot_pointer_accuracy: 0.7249 - slot_type_accuracy: 0.6731\n",
      "Epoch 1: saving model to model_epoch_01.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:102: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\n",
      "  return saving_lib.save_model(model, filepath)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmpmhv10752\\latest_checkpoint.h5:  85%|████████▍ | 550M/649M [00:13<00:00, 117MiB/s]  WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmpmhv10752\\latest_checkpoint.h5:  86%|████████▋ | 560M/649M [00:13<00:00, 117MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmpmhv10752\\latest_checkpoint.h5:  88%|████████▊ | 570M/649M [00:13<00:00, 117MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmpmhv10752\\latest_checkpoint.h5:  89%|████████▉ | 580M/649M [00:13<00:00, 150MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmpmhv10752\\latest_checkpoint.h5:  91%|█████████ | 590M/649M [00:13<00:00, 150MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmpmhv10752\\latest_checkpoint.h5:  92%|█████████▏| 600M/649M [00:13<00:00, 108MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmpmhv10752\\latest_checkpoint.h5:  94%|█████████▍| 610M/649M [00:13<00:00, 108MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmpmhv10752\\latest_checkpoint.h5:  96%|█████████▌| 620M/649M [00:13<00:00, 97.9MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmpmhv10752\\latest_checkpoint.h5:  97%|█████████▋| 630M/649M [00:14<00:00, 65.9MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmpmhv10752\\latest_checkpoint.h5:  99%|█████████▊| 640M/649M [00:14<00:00, 44.4MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmpmhv10752\\latest_checkpoint.h5: 100%|██████████| 649M/649M [00:15<00:00, 45.1MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2493s\u001b[0m 3s/step - intent_accuracy: 0.2509 - loss: 23.8885 - phantom_slot_actionable_accuracy: 0.2112 - phantom_slot_intent_accuracy: 0.6451 - phantom_slot_pointer_accuracy: 0.5457 - phantom_slot_target_accuracy: 0.1594 - slot_actionable_accuracy: 0.0115 - slot_intent_accuracy: 0.7784 - slot_pointer_accuracy: 0.7251 - slot_type_accuracy: 0.6733 - val_intent_accuracy: 0.4112 - val_loss: 98.5731 - val_phantom_slot_actionable_accuracy: 0.8157 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9333 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0137 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 5.0000e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.4063 - loss: 44.5784 - phantom_slot_actionable_accuracy: 0.2019 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9333 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0123 - slot_intent_accuracy: 0.9890 - slot_pointer_accuracy: 0.9925 - slot_type_accuracy: 0.9115\n",
      "Epoch 2: saving model to model_epoch_02.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2373s\u001b[0m 3s/step - intent_accuracy: 0.4063 - loss: 44.5854 - phantom_slot_actionable_accuracy: 0.2019 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9333 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0123 - slot_intent_accuracy: 0.9890 - slot_pointer_accuracy: 0.9925 - slot_type_accuracy: 0.9115 - val_intent_accuracy: 0.4340 - val_loss: 99.6485 - val_phantom_slot_actionable_accuracy: 0.0238 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9333 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0099 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 5.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.4361 - loss: 74.7387 - phantom_slot_actionable_accuracy: 0.1931 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9334 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0111 - slot_intent_accuracy: 0.9887 - slot_pointer_accuracy: 0.9923 - slot_type_accuracy: 0.9105\n",
      "Epoch 3: saving model to model_epoch_03.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmptxzcm2ag\\latest_checkpoint.h5:  85%|████████▍ | 550M/649M [00:09<00:01, 89.3MiB/s] WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmptxzcm2ag\\latest_checkpoint.h5:  88%|████████▊ | 570M/649M [00:09<00:00, 102MiB/s] WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmptxzcm2ag\\latest_checkpoint.h5:  89%|████████▉ | 580M/649M [00:09<00:00, 116MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmptxzcm2ag\\latest_checkpoint.h5:  91%|█████████ | 590M/649M [00:09<00:00, 116MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmptxzcm2ag\\latest_checkpoint.h5:  92%|█████████▏| 600M/649M [00:10<00:00, 107MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmptxzcm2ag\\latest_checkpoint.h5:  94%|█████████▍| 610M/649M [00:10<00:00, 107MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmptxzcm2ag\\latest_checkpoint.h5:  96%|█████████▌| 620M/649M [00:10<00:00, 94.8MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmptxzcm2ag\\latest_checkpoint.h5:  97%|█████████▋| 630M/649M [00:10<00:00, 94.8MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmptxzcm2ag\\latest_checkpoint.h5:  99%|█████████▊| 640M/649M [00:10<00:00, 75.5MiB/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmptxzcm2ag\\latest_checkpoint.h5: 100%|██████████| 649M/649M [00:11<00:00, 58.2MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2317s\u001b[0m 3s/step - intent_accuracy: 0.4361 - loss: 74.7476 - phantom_slot_actionable_accuracy: 0.1931 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9334 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0111 - slot_intent_accuracy: 0.9887 - slot_pointer_accuracy: 0.9923 - slot_type_accuracy: 0.9105 - val_intent_accuracy: 0.4317 - val_loss: 60.3561 - val_phantom_slot_actionable_accuracy: 0.0012 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9902 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0084 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 5.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.4631 - loss: 80.2630 - phantom_slot_actionable_accuracy: 0.1959 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9695 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0091 - slot_intent_accuracy: 0.9891 - slot_pointer_accuracy: 0.9925 - slot_type_accuracy: 0.9106\n",
      "Epoch 4: saving model to model_epoch_04.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2191s\u001b[0m 3s/step - intent_accuracy: 0.4631 - loss: 80.3066 - phantom_slot_actionable_accuracy: 0.1959 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9695 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0091 - slot_intent_accuracy: 0.9891 - slot_pointer_accuracy: 0.9925 - slot_type_accuracy: 0.9106 - val_intent_accuracy: 0.4847 - val_loss: 83.8441 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9333 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0069 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 5.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.4874 - loss: 171.1256 - phantom_slot_actionable_accuracy: 0.1898 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9333 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0110 - slot_intent_accuracy: 0.9892 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9099\n",
      "Epoch 5: saving model to model_epoch_05.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2188s\u001b[0m 3s/step - intent_accuracy: 0.4874 - loss: 171.0966 - phantom_slot_actionable_accuracy: 0.1898 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9333 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0110 - slot_intent_accuracy: 0.9892 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9099 - val_intent_accuracy: 0.4899 - val_loss: 95.2432 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9333 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0030 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 5.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5100 - loss: 128.4143 - phantom_slot_actionable_accuracy: 0.1896 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9383 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0119 - slot_intent_accuracy: 0.9891 - slot_pointer_accuracy: 0.9925 - slot_type_accuracy: 0.9110\n",
      "Epoch 6: saving model to model_epoch_06.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2188s\u001b[0m 3s/step - intent_accuracy: 0.5100 - loss: 128.4454 - phantom_slot_actionable_accuracy: 0.1896 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9383 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0119 - slot_intent_accuracy: 0.9891 - slot_pointer_accuracy: 0.9925 - slot_type_accuracy: 0.9110 - val_intent_accuracy: 0.5128 - val_loss: 129.1004 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9992 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0166 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 5.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5204 - loss: 197.1690 - phantom_slot_actionable_accuracy: 0.1986 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9991 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0128 - slot_intent_accuracy: 0.9890 - slot_pointer_accuracy: 0.9925 - slot_type_accuracy: 0.9106\n",
      "Epoch 7: saving model to model_epoch_07.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2189s\u001b[0m 3s/step - intent_accuracy: 0.5204 - loss: 197.1437 - phantom_slot_actionable_accuracy: 0.1986 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9991 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0128 - slot_intent_accuracy: 0.9890 - slot_pointer_accuracy: 0.9925 - slot_type_accuracy: 0.9106 - val_intent_accuracy: 0.5223 - val_loss: 125.5642 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9989 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0019 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 5.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5325 - loss: 166.9455 - phantom_slot_actionable_accuracy: 0.1909 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9989 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0122 - slot_intent_accuracy: 0.9888 - slot_pointer_accuracy: 0.9923 - slot_type_accuracy: 0.9101\n",
      "Epoch 8: saving model to model_epoch_08.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2190s\u001b[0m 3s/step - intent_accuracy: 0.5325 - loss: 166.9784 - phantom_slot_actionable_accuracy: 0.1909 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9989 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0122 - slot_intent_accuracy: 0.9888 - slot_pointer_accuracy: 0.9923 - slot_type_accuracy: 0.9101 - val_intent_accuracy: 0.5345 - val_loss: 175.9119 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9990 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0127 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 4.8500e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5400 - loss: 244.5097 - phantom_slot_actionable_accuracy: 0.1894 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9989 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0114 - slot_intent_accuracy: 0.9888 - slot_pointer_accuracy: 0.9923 - slot_type_accuracy: 0.9106\n",
      "Epoch 9: saving model to model_epoch_09.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2191s\u001b[0m 3s/step - intent_accuracy: 0.5400 - loss: 244.4536 - phantom_slot_actionable_accuracy: 0.1894 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9989 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0114 - slot_intent_accuracy: 0.9888 - slot_pointer_accuracy: 0.9923 - slot_type_accuracy: 0.9106 - val_intent_accuracy: 0.5396 - val_loss: 142.9483 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9990 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0147 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 4.7045e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5436 - loss: 191.5485 - phantom_slot_actionable_accuracy: 0.1929 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9990 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0116 - slot_intent_accuracy: 0.9890 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9111\n",
      "Epoch 10: saving model to model_epoch_10.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2196s\u001b[0m 3s/step - intent_accuracy: 0.5436 - loss: 191.6007 - phantom_slot_actionable_accuracy: 0.1929 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9990 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0116 - slot_intent_accuracy: 0.9890 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9111 - val_intent_accuracy: 0.5437 - val_loss: 132.6075 - val_phantom_slot_actionable_accuracy: 0.9997 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9988 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0180 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 4.5634e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5492 - loss: 173.6930 - phantom_slot_actionable_accuracy: 0.1989 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9990 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0108 - slot_intent_accuracy: 0.9891 - slot_pointer_accuracy: 0.9926 - slot_type_accuracy: 0.9114\n",
      "Epoch 11: saving model to model_epoch_11.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2255s\u001b[0m 3s/step - intent_accuracy: 0.5492 - loss: 173.7617 - phantom_slot_actionable_accuracy: 0.1989 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9990 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0108 - slot_intent_accuracy: 0.9891 - slot_pointer_accuracy: 0.9926 - slot_type_accuracy: 0.9114 - val_intent_accuracy: 0.5471 - val_loss: 114.1478 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9987 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0022 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 4.4265e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5552 - loss: 306.1930 - phantom_slot_actionable_accuracy: 0.1901 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9985 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0104 - slot_intent_accuracy: 0.9891 - slot_pointer_accuracy: 0.9925 - slot_type_accuracy: 0.9106\n",
      "Epoch 12: saving model to model_epoch_12.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2195s\u001b[0m 3s/step - intent_accuracy: 0.5552 - loss: 306.1103 - phantom_slot_actionable_accuracy: 0.1901 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9985 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0104 - slot_intent_accuracy: 0.9891 - slot_pointer_accuracy: 0.9925 - slot_type_accuracy: 0.9106 - val_intent_accuracy: 0.5513 - val_loss: 394.0614 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9988 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0017 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 4.2937e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5541 - loss: 222.6603 - phantom_slot_actionable_accuracy: 0.1949 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9990 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0117 - slot_intent_accuracy: 0.9893 - slot_pointer_accuracy: 0.9926 - slot_type_accuracy: 0.9108\n",
      "Epoch 13: saving model to model_epoch_13.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2192s\u001b[0m 3s/step - intent_accuracy: 0.5541 - loss: 222.6917 - phantom_slot_actionable_accuracy: 0.1949 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9990 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0117 - slot_intent_accuracy: 0.9893 - slot_pointer_accuracy: 0.9926 - slot_type_accuracy: 0.9108 - val_intent_accuracy: 0.5544 - val_loss: 147.6241 - val_phantom_slot_actionable_accuracy: 0.9909 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9987 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0153 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 4.1649e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5555 - loss: 208.6518 - phantom_slot_actionable_accuracy: 0.1968 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9989 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0107 - slot_intent_accuracy: 0.9892 - slot_pointer_accuracy: 0.9925 - slot_type_accuracy: 0.9106\n",
      "Epoch 14: saving model to model_epoch_14.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2192s\u001b[0m 3s/step - intent_accuracy: 0.5555 - loss: 208.6979 - phantom_slot_actionable_accuracy: 0.1968 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9989 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0107 - slot_intent_accuracy: 0.9892 - slot_pointer_accuracy: 0.9925 - slot_type_accuracy: 0.9106 - val_intent_accuracy: 0.5555 - val_loss: 146.2648 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9988 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0127 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 4.0399e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5525 - loss: 210.6642 - phantom_slot_actionable_accuracy: 0.1906 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9990 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0105 - slot_intent_accuracy: 0.9890 - slot_pointer_accuracy: 0.9926 - slot_type_accuracy: 0.9104\n",
      "Epoch 15: saving model to model_epoch_15.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2194s\u001b[0m 3s/step - intent_accuracy: 0.5525 - loss: 210.7275 - phantom_slot_actionable_accuracy: 0.1906 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9990 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0105 - slot_intent_accuracy: 0.9890 - slot_pointer_accuracy: 0.9926 - slot_type_accuracy: 0.9104 - val_intent_accuracy: 0.5542 - val_loss: 164.2316 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9989 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0127 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 3.9187e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5604 - loss: 208.0602 - phantom_slot_actionable_accuracy: 0.1934 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9991 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0135 - slot_intent_accuracy: 0.9888 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9107\n",
      "Epoch 16: saving model to model_epoch_16.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2216s\u001b[0m 3s/step - intent_accuracy: 0.5604 - loss: 208.1328 - phantom_slot_actionable_accuracy: 0.1934 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9991 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0135 - slot_intent_accuracy: 0.9888 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9107 - val_intent_accuracy: 0.5574 - val_loss: 346.1590 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9989 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0017 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 3.8012e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5607 - loss: 252.0364 - phantom_slot_actionable_accuracy: 0.1905 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9991 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0102 - slot_intent_accuracy: 0.9892 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9115\n",
      "Epoch 17: saving model to model_epoch_17.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2202s\u001b[0m 3s/step - intent_accuracy: 0.5607 - loss: 252.0608 - phantom_slot_actionable_accuracy: 0.1905 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9991 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0102 - slot_intent_accuracy: 0.9892 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9115 - val_intent_accuracy: 0.5585 - val_loss: 212.9991 - val_phantom_slot_actionable_accuracy: 1.0000 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9990 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0127 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 3.6871e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5576 - loss: 263.4792 - phantom_slot_actionable_accuracy: 0.1977 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9992 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0123 - slot_intent_accuracy: 0.9889 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9103\n",
      "Epoch 18: saving model to model_epoch_18.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2199s\u001b[0m 3s/step - intent_accuracy: 0.5576 - loss: 263.4766 - phantom_slot_actionable_accuracy: 0.1977 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9992 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0123 - slot_intent_accuracy: 0.9889 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9103 - val_intent_accuracy: 0.5603 - val_loss: 162.8761 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9992 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0128 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 3.5765e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5661 - loss: 226.2966 - phantom_slot_actionable_accuracy: 0.1975 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9993 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0113 - slot_intent_accuracy: 0.9892 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9111\n",
      "Epoch 19: saving model to model_epoch_19.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2202s\u001b[0m 3s/step - intent_accuracy: 0.5661 - loss: 226.3440 - phantom_slot_actionable_accuracy: 0.1975 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9993 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0113 - slot_intent_accuracy: 0.9892 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9111 - val_intent_accuracy: 0.5585 - val_loss: 162.4736 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9993 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0066 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 3.4692e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5666 - loss: 314.5500 - phantom_slot_actionable_accuracy: 0.1997 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9994 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0136 - slot_intent_accuracy: 0.9889 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9099\n",
      "Epoch 20: saving model to model_epoch_20.keras\n",
      "\u001b[1m868/868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2199s\u001b[0m 3s/step - intent_accuracy: 0.5666 - loss: 314.4826 - phantom_slot_actionable_accuracy: 0.1997 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9994 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0136 - slot_intent_accuracy: 0.9889 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9099 - val_intent_accuracy: 0.5608 - val_loss: 197.2517 - val_phantom_slot_actionable_accuracy: 0.0000e+00 - val_phantom_slot_intent_accuracy: 1.0000 - val_phantom_slot_pointer_accuracy: 0.9993 - val_phantom_slot_target_accuracy: 0.8000 - val_slot_actionable_accuracy: 0.0149 - val_slot_intent_accuracy: 0.9893 - val_slot_pointer_accuracy: 0.9924 - val_slot_type_accuracy: 0.9101 - learning_rate: 3.3651e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/06 06:21:44 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: tuple index out of range\n",
      "2024/10/06 06:21:44 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2024/10/06 06:22:04 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\falkt\\AppData\\Local\\Temp\\tmpaqo23f4y\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.17.0', 'cloudpickle==3.0.0']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/10/06 06:22:04 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Uploading artifacts:  71%|███████▏  | 5/7 [00:00<00:01,  1.32it/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com. Connection pool size: 10\n",
      "Uploading C:\\Users\\falkt\\AppData\\Local\\Temp\\tmpaqo23f4y\\model\\data\\model.keras: 100%|██████████| 649M/649M [00:14<00:00, 45.9MiB/s]\n",
      "Uploading artifacts: 100%|██████████| 7/7 [00:16<00:00,  2.33s/it]\n",
      "Uploading artifacts: 100%|██████████| 2/2 [00:01<00:00,  1.68it/s]\n",
      "2024/10/06 06:22:22 INFO mlflow.tracking._tracking_service.client: 🏃 View run upbeat-eel-438 at: https://community.cloud.databricks.com/ml/experiments/1400101170489055/runs/e0a1be0dafc047ea961a0d7762da2711.\n",
      "2024/10/06 06:22:22 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://community.cloud.databricks.com/ml/experiments/1400101170489055.\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "opt = Adam(learning_rate=5e-5, epsilon=1e-08)\n",
    "\n",
    "# learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 7:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.97\n",
    "learning_rate_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=\"model_epoch_{epoch:02d}.keras\",  # Save the model with the epoch number in the filename\n",
    "    save_freq='epoch',\n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "# loss functions\n",
    "losses = [\n",
    "    CategoricalCrossentropy(name=\"intent_loss\"), \n",
    "    SparseCategoricalCrossentropy(from_logits=True, name=\"slot_type_loss\"),\n",
    "    SparseCategoricalCrossentropy(from_logits=True, name=\"slot_intent_loss\"),\n",
    "    BinaryCrossentropy(name=\"slot_actionable_loss\"), \n",
    "    SparseCategoricalCrossentropy(from_logits=True, name=\"slot_pointer_loss\"),\n",
    "    SparseCategoricalCrossentropy(from_logits=True, name=\"phantom_slot_target_loss\"),\n",
    "    SparseCategoricalCrossentropy(from_logits=True, name=\"phantom_slot_intent_loss\"),\n",
    "    BinaryCrossentropy(name=\"phantom_slot_actionable_loss\"), \n",
    "    SparseCategoricalCrossentropy(from_logits=True, name=\"phantom_slot_pointer_loss\"),\n",
    "]\n",
    "# metrics\n",
    "metrics = [\n",
    "    CategoricalAccuracy(name=\"intent_accuracy\"), \n",
    "    SparseCategoricalAccuracy(name=\"slot_type_accuracy\"),\n",
    "    SparseCategoricalAccuracy(name=\"slot_intent_accuracy\"),\n",
    "    BinaryAccuracy(name=\"slot_actionable_accuracy\"), \n",
    "    SparseCategoricalAccuracy(name=\"slot_pointer_accuracy\"),\n",
    "    SparseCategoricalAccuracy(name=\"phantom_slot_target_accuracy\"),\n",
    "    SparseCategoricalAccuracy(name=\"phantom_slot_intent_accuracy\"),\n",
    "    BinaryAccuracy(name=\"phantom_slot_actionable_accuracy\"), \n",
    "    SparseCategoricalAccuracy(name=\"phantom_slot_pointer_accuracy\"),\n",
    "]\n",
    "\n",
    "# compile model\n",
    "joint_model.compile(optimizer=opt, loss=losses, metrics=metrics)\n",
    "\n",
    "history = joint_model.fit(\n",
    "    x=inputs_train, \n",
    "    y=(intentOutputs_train, slot_type_map_train, slot_intent_map_train, slot_action_map_train, slot_pointers_map_train, phantom_target_map_train, phantom_intent_map_train, phantom_action_map_train, phantom_pointers_map_train), \n",
    "    validation_data=(inputs_val, (intentOutputs_val, slot_type_map_val, slot_intent_map_val, slot_action_map_val, slot_pointers_map_val, phantom_target_map_val, phantom_intent_map_val, phantom_action_map_val, phantom_pointers_map_val)), \n",
    "    epochs=20, \n",
    "    batch_size=24,\n",
    "    shuffle=True, \n",
    "    callbacks=[learning_rate_callback, checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "\n",
    "keep in mind -> test slot accuracy will be higher than reality.\n",
    "        Because 90% of the data points are 0, it can just guess 0 and be right 85% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 2s/step - intent_accuracy: 0.5786 - loss: 200.7589 - phantom_slot_actionable_accuracy: 0.0000e+00 - phantom_slot_intent_accuracy: 1.0000 - phantom_slot_pointer_accuracy: 0.9996 - phantom_slot_target_accuracy: 0.8000 - slot_actionable_accuracy: 0.0166 - slot_intent_accuracy: 0.9887 - slot_pointer_accuracy: 0.9924 - slot_type_accuracy: 0.9114\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_loss, test_slot_acc, test_intent_acc \u001b[38;5;241m=\u001b[39m joint_model\u001b[38;5;241m.\u001b[39mevaluate(x\u001b[38;5;241m=\u001b[39minputs_test, y\u001b[38;5;241m=\u001b[39m(intentOutputs_test, slot_type_map_test, slot_intent_map_test, slot_action_map_test, slot_pointers_map_test, phantom_target_map_test, phantom_intent_map_test, phantom_action_map_test, phantom_pointers_map_test), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Slot Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_slot_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Intent Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_intent_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "test_loss, test_slot_acc, test_intent_acc = joint_model.evaluate(x=inputs_test, y=(intentOutputs_test, slot_type_map_test, slot_intent_map_test, slot_action_map_test, slot_pointers_map_test, phantom_target_map_test, phantom_intent_map_test, phantom_action_map_test, phantom_pointers_map_test), batch_size=24)\n",
    "\n",
    "print(f\"Test Slot Accuracy: {test_slot_acc}\")\n",
    "print(f\"Test Intent Accuracy: {test_intent_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlu(text, tokenizer, model, intent_names, slot_names):\n",
    "    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n",
    "    outputs = model(inputs)\n",
    "    slot_logits, intent_logits = outputs\n",
    "\n",
    "    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, :]\n",
    "    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n",
    "\n",
    "    info = {\"intent\": intent_names[intent_id], \"slots\": {}}\n",
    "\n",
    "    out_dict = {}\n",
    "    # get all slot names and add to out_dict as keys\n",
    "    predicted_slots = set([re.sub(r'^[BI]-', '', slot_names[s]) for s in slot_ids if s != 0])\n",
    "    for ps in predicted_slots:\n",
    "      out_dict[ps] = []\n",
    "\n",
    "    # check if the text starts with a small letter\n",
    "    if text[0].islower():\n",
    "      tokens = tokenizer.tokenize(text, add_special_tokens=True)\n",
    "    else:\n",
    "      tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # process sequence output for slots\n",
    "    for token, slot_id in zip(tokens, slot_ids):\n",
    "      # add all to out_dict\n",
    "      slot_name = slot_names[slot_id]\n",
    "\n",
    "      if slot_name == \"[PAD]\":\n",
    "        continue\n",
    "\n",
    "      # collect tokens\n",
    "      collected_tokens = [token]\n",
    "      idx = tokens.index(token)\n",
    "\n",
    "      # see if it starts with ##\n",
    "      # then it belongs to the previous token\n",
    "      if token.startswith(\"##\"):\n",
    "\n",
    "        # check if the token already exists or not\n",
    "        if tokens[idx - 1] not in out_dict[slot_name]:\n",
    "          collected_tokens.insert(0, tokens[idx - 1])\n",
    "\n",
    "      # add collected tokens to slots\n",
    "      out_dict[slot_name].extend(collected_tokens)\n",
    "\n",
    "    # process out_dict\n",
    "    for slot_name in out_dict:\n",
    "      tokens = out_dict[slot_name]\n",
    "      slot_value = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "      info[\"slots\"][slot_name] = slot_value.strip()\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = tf.keras.models.load_model('model_epoch_18.keras', custom_objects={'JointIntentAndSlotFillingModel': JointIntentAndSlotFillingModel})\n",
    "\n",
    "print(nlu(\"could you please change my reservation on december twenty first originally scheduled for twelve thirty in the morning to now be at four thirty five pm\", tokenizer, joint_model, intent_names, slot_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
