{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Bert Model for slot and intent classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'remove'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file found with that path!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# read from json file\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mread_train_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJERTmate_data.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m, in \u001b[0;36mread_train_json_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     29\u001b[0m text \u001b[38;5;241m=\u001b[39m data[k][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     30\u001b[0m slots \u001b[38;5;241m=\u001b[39m data[k][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslots\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 31\u001b[0m remove \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mremove\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     32\u001b[0m memory \u001b[38;5;241m=\u001b[39m data[k][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     34\u001b[0m temp \u001b[38;5;241m=\u001b[39m RawData(k, intent, text, slots, remove, memory)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'remove'"
     ]
    }
   ],
   "source": [
    "class RawData(object):\n",
    "    def __init__(self, id, intent, text, slots, memory):\n",
    "        self.id = id\n",
    "        self.intent = intent\n",
    "        self.text = text\n",
    "        self.slots = slots\n",
    "        self.memory = memory\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(json.dumps(self.__dict__, indent=2))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "reads json from data file\n",
    "returns a list containing DataInstance objects\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def read_train_json_file(filename):\n",
    "    if os.path.exists(filename):\n",
    "        intents = []\n",
    "\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "            for k in data.keys():\n",
    "                intent = data[k][\"intent\"]\n",
    "                text = data[k][\"text\"]\n",
    "                slots = data[k][\"slots\"]\n",
    "                memory = data[k][\"memory\"]\n",
    "\n",
    "                temp = RawData(k, intent, text, slots, memory)\n",
    "                intents.append(temp)\n",
    "\n",
    "        return intents\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No file found with that path!\")\n",
    "\n",
    "# read from json file\n",
    "data = read_train_json_file(\"JERTmate_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_intents = [d.intent for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_slots(data):\n",
    "    slots = []\n",
    "    phantom_slot = None\n",
    "    for slot in data:\n",
    "        if len(slot) > 3:\n",
    "            phantom_slot = slot[0:3]\n",
    "        else:\n",
    "            slots.extend(slot)\n",
    "\n",
    "    # Add padding\n",
    "    slots.extend(np.zeros(60 - len(slots)))\n",
    "\n",
    "    if phantom_slot is not None:\n",
    "        slots.extend(phantom_slot)\n",
    "    else:\n",
    "        slots.extend(np.zeros(3))\n",
    "        \n",
    "    return slots\n",
    "\n",
    "encoded_slots = []\n",
    "for data_point in data:\n",
    "    encoded_slots.append(encode_slots(data_point.slots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer and Encode Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(texts, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m encoded_texts \u001b[38;5;241m=\u001b[39m encode_texts(tokenizer, \u001b[43mdata\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#encode and pad texts\n",
    "def encode_texts(tokenizer, data):\n",
    "    texts = []\n",
    "    for data_point in data:\n",
    "        text = ''\n",
    "        for previous_sentence in data_point.memory:\n",
    "            text += previous_sentence.text + \" [SEP] \"\n",
    "\n",
    "        text += data_point.text\n",
    "        texts.append(text)\n",
    "        \n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "encoded_texts = encode_texts(tokenizer, data)\n",
    "\n",
    "#parse texts and memory into input data\n",
    "encoded_inputs = []\n",
    "for i in range(len(data)):\n",
    "    intent_memory = []\n",
    "    slot_memory = []\n",
    "\n",
    "    for sentence in data[i].memory:\n",
    "        intent_memory.append(sentence.intent)\n",
    "        slot_memory.extend(encode_slots(sentence.slots))\n",
    "\n",
    "    encoded_inputs.append([encoded_texts[i], intent_memory, slot_memory])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data - Train 80% | Validation 10% | Test 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_intent(encoded_intents, train_ratio, val_ratio, test_ratio, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    intent_indices = defaultdict(list)\n",
    "\n",
    "    # Group indices by intent\n",
    "    for idx, intent in enumerate(encoded_intents):\n",
    "        intent_indices[intent.numpy()].append(idx)\n",
    "\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # Split indices for each intent\n",
    "    for indices in intent_indices.values():\n",
    "        np.random.shuffle(indices)\n",
    "        n_total = len(indices)\n",
    "        n_train = int(n_total * train_ratio)\n",
    "        n_val = int(n_total * val_ratio)\n",
    "        \n",
    "        train_indices.extend(indices[:n_train])\n",
    "        val_indices.extend(indices[n_train:n_train + n_val])\n",
    "        test_indices.extend(indices[n_train + n_val:])\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "def split_and_shuffle(encoded_texts, encoded_slots, encoded_intents, train_ratio, val_ratio, test_ratio):\n",
    "    train_indices, val_indices, test_indices = split_data_by_intent(encoded_intents, train_ratio, val_ratio, test_ratio)\n",
    "\n",
    "    def extract_data(indices):\n",
    "        return {key: tf.gather(value, indices) for key, value in encoded_texts.items()}, tf.gather(encoded_slots, indices), tf.gather(encoded_intents, indices)\n",
    "\n",
    "    train_encoded_texts, train_encoded_slots, train_encoded_intents = extract_data(train_indices)\n",
    "    val_encoded_texts, val_encoded_slots, val_encoded_intents = extract_data(val_indices)\n",
    "    test_encoded_texts, test_encoded_slots, test_encoded_intents = extract_data(test_indices)\n",
    "\n",
    "    return (train_encoded_texts, train_encoded_slots, train_encoded_intents), (val_encoded_texts, val_encoded_slots, val_encoded_intents), (test_encoded_texts, test_encoded_slots, test_encoded_intents)\n",
    "\n",
    "(train_encoded_texts, train_encoded_slots, train_encoded_intents), (val_encoded_texts, val_encoded_slots, val_encoded_intents), (test_encoded_texts, test_encoded_slots, test_encoded_intents) = split_and_shuffle(encoded_inputs, encoded_slots, encoded_intents, 0.8, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "class JointIntentAndSlotFillingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, intent_num_labels=None, slot_num_labels=None, model_name=model_name, dropout_prob=0.1):\n",
    "        super().__init__(name=\"joint_intent_slot\")\n",
    "        self.bert = TFBertModel.from_pretrained(model_name)\n",
    "        self.dropout = Dropout(dropout_prob)\n",
    "        self.intent_classifier = Dense(intent_num_labels, name=\"intent_classifier\")\n",
    "        self.slot_classifier = Dense(slot_num_labels, name=\"slot_classifier\")\n",
    "\n",
    "    def __call__(self, inputs, **kwargs):\n",
    "        # two outputs from BERT\n",
    "        trained_bert = self.bert(inputs[0], **kwargs)\n",
    "        pooled_output = trained_bert.pooler_output\n",
    "        sequence_output = trained_bert.last_hidden_state\n",
    "\n",
    "        # slot_filling / classification\n",
    "        slot_input = tf.concat([sequence_output, tf.reshape(inputs[1:], [-1])], axis=-1) # Combine sequence output and all memory data\n",
    "\n",
    "        slot_input = self.dropout(slot_input, training=kwargs.get(\"training\", False))\n",
    "        slot_logits = self.slot_classifier(slot_input)\n",
    "\n",
    "        # intent classification\n",
    "        intent_input = tf.concat([pooled_output, inputs[1]], axis=-1) # Combine pooled output and memory intent data\n",
    "\n",
    "        intent_input = self.dropout(intent_input, training=kwargs.get(\"training\", False))\n",
    "        intent_logits = self.intent_classifier(intent_input)\n",
    "\n",
    "        return slot_logits, intent_logits, sequence_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(JointIntentAndSlotFillingModel, self).get_config()\n",
    "        config.update({\n",
    "            \"dropout\": self.dropout_prob,\n",
    "            \"intent_num_labels\": self.intent_num_labels,\n",
    "            \"slot_num_labels\": self.slot_num_labels,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "joint_model = JointIntentAndSlotFillingModel(intent_num_labels=len(intent_map), slot_num_labels=len(slot_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.3142 - loss: 2.5271 - slot_accuracy: 0.9288\n",
      "Epoch 1: saving model to model_epoch_01.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7231s\u001b[0m 3s/step - intent_accuracy: 0.3142 - loss: 2.5270 - slot_accuracy: 0.9288 - val_intent_accuracy: 0.4787 - val_loss: 1.7735 - val_slot_accuracy: 0.9645 - learning_rate: 5.0000e-05\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:102: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\n",
      "  return saving_lib.save_model(model, filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.5354 - loss: 1.6793 - slot_accuracy: 0.9663\n",
      "Epoch 2: saving model to model_epoch_02.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7147s\u001b[0m 3s/step - intent_accuracy: 0.5354 - loss: 1.6793 - slot_accuracy: 0.9663 - val_intent_accuracy: 0.6088 - val_loss: 1.4713 - val_slot_accuracy: 0.9712 - learning_rate: 5.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.6301 - loss: 1.4190 - slot_accuracy: 0.9724\n",
      "Epoch 3: saving model to model_epoch_03.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7028s\u001b[0m 3s/step - intent_accuracy: 0.6301 - loss: 1.4190 - slot_accuracy: 0.9724 - val_intent_accuracy: 0.6665 - val_loss: 1.2910 - val_slot_accuracy: 0.9754 - learning_rate: 5.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.6813 - loss: 1.2516 - slot_accuracy: 0.9760\n",
      "Epoch 4: saving model to model_epoch_04.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7001s\u001b[0m 3s/step - intent_accuracy: 0.6813 - loss: 1.2516 - slot_accuracy: 0.9760 - val_intent_accuracy: 0.7023 - val_loss: 1.1652 - val_slot_accuracy: 0.9777 - learning_rate: 5.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.7164 - loss: 1.1361 - slot_accuracy: 0.9778\n",
      "Epoch 5: saving model to model_epoch_05.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7024s\u001b[0m 3s/step - intent_accuracy: 0.7164 - loss: 1.1361 - slot_accuracy: 0.9778 - val_intent_accuracy: 0.7357 - val_loss: 1.0699 - val_slot_accuracy: 0.9792 - learning_rate: 5.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.7442 - loss: 1.0372 - slot_accuracy: 0.9793\n",
      "Epoch 6: saving model to model_epoch_06.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7008s\u001b[0m 3s/step - intent_accuracy: 0.7442 - loss: 1.0372 - slot_accuracy: 0.9793 - val_intent_accuracy: 0.7607 - val_loss: 0.9952 - val_slot_accuracy: 0.9801 - learning_rate: 5.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.7647 - loss: 0.9673 - slot_accuracy: 0.9802\n",
      "Epoch 7: saving model to model_epoch_07.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6988s\u001b[0m 3s/step - intent_accuracy: 0.7647 - loss: 0.9673 - slot_accuracy: 0.9802 - val_intent_accuracy: 0.7680 - val_loss: 0.9317 - val_slot_accuracy: 0.9808 - learning_rate: 5.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.7801 - loss: 0.9113 - slot_accuracy: 0.9808\n",
      "Epoch 8: saving model to model_epoch_08.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6975s\u001b[0m 3s/step - intent_accuracy: 0.7801 - loss: 0.9113 - slot_accuracy: 0.9808 - val_intent_accuracy: 0.7874 - val_loss: 0.8793 - val_slot_accuracy: 0.9813 - learning_rate: 5.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.7980 - loss: 0.8553 - slot_accuracy: 0.9812\n",
      "Epoch 9: saving model to model_epoch_09.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6941s\u001b[0m 3s/step - intent_accuracy: 0.7980 - loss: 0.8553 - slot_accuracy: 0.9812 - val_intent_accuracy: 0.8003 - val_loss: 0.8343 - val_slot_accuracy: 0.9817 - learning_rate: 5.0000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8104 - loss: 0.8085 - slot_accuracy: 0.9817\n",
      "Epoch 10: saving model to model_epoch_10.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7056s\u001b[0m 3s/step - intent_accuracy: 0.8104 - loss: 0.8085 - slot_accuracy: 0.9817 - val_intent_accuracy: 0.8087 - val_loss: 0.7938 - val_slot_accuracy: 0.9821 - learning_rate: 5.0000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8221 - loss: 0.7699 - slot_accuracy: 0.9820\n",
      "Epoch 11: saving model to model_epoch_11.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7133s\u001b[0m 3s/step - intent_accuracy: 0.8221 - loss: 0.7699 - slot_accuracy: 0.9820 - val_intent_accuracy: 0.8213 - val_loss: 0.7583 - val_slot_accuracy: 0.9824 - learning_rate: 5.0000e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8323 - loss: 0.7345 - slot_accuracy: 0.9823\n",
      "Epoch 12: saving model to model_epoch_12.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7215s\u001b[0m 3s/step - intent_accuracy: 0.8323 - loss: 0.7345 - slot_accuracy: 0.9823 - val_intent_accuracy: 0.8298 - val_loss: 0.7281 - val_slot_accuracy: 0.9826 - learning_rate: 5.0000e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8409 - loss: 0.7015 - slot_accuracy: 0.9826\n",
      "Epoch 13: saving model to model_epoch_13.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7174s\u001b[0m 3s/step - intent_accuracy: 0.8409 - loss: 0.7015 - slot_accuracy: 0.9826 - val_intent_accuracy: 0.8358 - val_loss: 0.6997 - val_slot_accuracy: 0.9829 - learning_rate: 5.0000e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8457 - loss: 0.6803 - slot_accuracy: 0.9827\n",
      "Epoch 14: saving model to model_epoch_14.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7134s\u001b[0m 3s/step - intent_accuracy: 0.8457 - loss: 0.6803 - slot_accuracy: 0.9827 - val_intent_accuracy: 0.8436 - val_loss: 0.6764 - val_slot_accuracy: 0.9830 - learning_rate: 4.6000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8505 - loss: 0.6594 - slot_accuracy: 0.9828\n",
      "Epoch 15: saving model to model_epoch_15.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7108s\u001b[0m 3s/step - intent_accuracy: 0.8505 - loss: 0.6594 - slot_accuracy: 0.9828 - val_intent_accuracy: 0.8465 - val_loss: 0.6569 - val_slot_accuracy: 0.9832 - learning_rate: 4.2320e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8544 - loss: 0.6421 - slot_accuracy: 0.9830\n",
      "Epoch 16: saving model to model_epoch_16.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7128s\u001b[0m 3s/step - intent_accuracy: 0.8544 - loss: 0.6421 - slot_accuracy: 0.9830 - val_intent_accuracy: 0.8520 - val_loss: 0.6392 - val_slot_accuracy: 0.9832 - learning_rate: 3.8934e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8614 - loss: 0.6206 - slot_accuracy: 0.9832\n",
      "Epoch 17: saving model to model_epoch_17.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7137s\u001b[0m 3s/step - intent_accuracy: 0.8614 - loss: 0.6206 - slot_accuracy: 0.9832 - val_intent_accuracy: 0.8579 - val_loss: 0.6244 - val_slot_accuracy: 0.9834 - learning_rate: 3.5820e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - intent_accuracy: 0.8651 - loss: 0.6069 - slot_accuracy: 0.9832\n",
      "Epoch 18: saving model to model_epoch_18.keras\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7168s\u001b[0m 3s/step - intent_accuracy: 0.8651 - loss: 0.6069 - slot_accuracy: 0.9832 - val_intent_accuracy: 0.8594 - val_loss: 0.6118 - val_slot_accuracy: 0.9835 - learning_rate: 3.2954e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m2320/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17:33\u001b[0m 2s/step - intent_accuracy: 0.8682 - loss: 0.5948 - slot_accuracy: 0.9833"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "opt = Adam(learning_rate=5e-5, epsilon=1e-08)\n",
    "\n",
    "# learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 13:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.97\n",
    "learning_rate_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=\"model_epoch_{epoch:02d}.keras\",  # Save the model with the epoch number in the filename\n",
    "    save_freq='epoch',\n",
    "    verbose=2 # Save the model every 5 epochs\n",
    ")\n",
    "\n",
    "# two losses, one for slots, another for intents\n",
    "losses = [SparseCategoricalCrossentropy(from_logits=True, name=\"slot_loss\"), SparseCategoricalCrossentropy(from_logits=True, name=\"intent_loss\")]\n",
    "metrics = [SparseCategoricalAccuracy(name=\"slot_accuracy\"), SparseCategoricalAccuracy(name=\"intent_accuracy\")]\n",
    "\n",
    "# compile model\n",
    "joint_model.compile(optimizer=opt, loss=losses, metrics=metrics)\n",
    "\n",
    "history = joint_model.fit(\n",
    "    x=train_encoded_texts, \n",
    "    y=(train_encoded_slots, train_encoded_intents), \n",
    "    validation_data=(val_encoded_texts, (val_encoded_slots, val_encoded_intents)), \n",
    "    epochs=20, \n",
    "    batch_size=32,\n",
    "    shuffle=True, \n",
    "    callbacks=[learning_rate_callback, checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "\n",
    "keep in mind -> test slot accuracy will be higher than reality.\n",
    "        Because 90% of the data points are 0, it can just guess 0 and be right 85% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must call `compile()` before using the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_loss, test_slot_acc, test_intent_acc \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_encoded_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_encoded_slots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_encoded_intents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Slot Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_slot_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Intent Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_intent_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py:993\u001b[0m, in \u001b[0;36mTrainer._assert_compile_called\u001b[1;34m(self, method_name)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    992\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: You must call `compile()` before using the model."
     ]
    }
   ],
   "source": [
    "test_loss, test_slot_acc, test_intent_acc = joint_model.evaluate(x=test_encoded_texts, y=(test_encoded_slots, test_encoded_intents), batch_size=32)\n",
    "\n",
    "print(f\"Test Slot Accuracy: {test_slot_acc}\")\n",
    "print(f\"Test Intent Accuracy: {test_intent_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlu(text, tokenizer, model, intent_names, slot_names):\n",
    "    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n",
    "    outputs = model(inputs)\n",
    "    slot_logits, intent_logits = outputs\n",
    "\n",
    "    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, :]\n",
    "    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n",
    "\n",
    "    info = {\"intent\": intent_names[intent_id], \"slots\": {}}\n",
    "\n",
    "    out_dict = {}\n",
    "    # get all slot names and add to out_dict as keys\n",
    "    predicted_slots = set([re.sub(r'^[BI]-', '', slot_names[s]) for s in slot_ids if s != 0])\n",
    "    for ps in predicted_slots:\n",
    "      out_dict[ps] = []\n",
    "\n",
    "    # check if the text starts with a small letter\n",
    "    if text[0].islower():\n",
    "      tokens = tokenizer.tokenize(text, add_special_tokens=True)\n",
    "    else:\n",
    "      tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # process sequence output for slots\n",
    "    for token, slot_id in zip(tokens, slot_ids):\n",
    "      # add all to out_dict\n",
    "      slot_name = slot_names[slot_id]\n",
    "\n",
    "      if slot_name == \"[PAD]\":\n",
    "        continue\n",
    "\n",
    "      # collect tokens\n",
    "      collected_tokens = [token]\n",
    "      idx = tokens.index(token)\n",
    "\n",
    "      # see if it starts with ##\n",
    "      # then it belongs to the previous token\n",
    "      if token.startswith(\"##\"):\n",
    "\n",
    "        # check if the token already exists or not\n",
    "        if tokens[idx - 1] not in out_dict[slot_name]:\n",
    "          collected_tokens.insert(0, tokens[idx - 1])\n",
    "\n",
    "      # add collected tokens to slots\n",
    "      out_dict[slot_name].extend(collected_tokens)\n",
    "\n",
    "    # process out_dict\n",
    "    for slot_name in out_dict:\n",
    "      tokens = out_dict[slot_name]\n",
    "      slot_value = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "      info[\"slots\"][slot_name] = slot_value.strip()\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': 'add_res', 'slots': {'NAME': 'thirty', 'ITEM': 'could you please reservation december originally scheduled twelve thirty in morning at four five', 'DRINK': '[CLS] change twenty', 'DATE': 'my on for the now be', 'SIZE': 'first to pm', 'NUMBER': '[SEP]'}}\n"
     ]
    }
   ],
   "source": [
    "#loaded_model = tf.keras.models.load_model('model_epoch_18.keras', custom_objects={'JointIntentAndSlotFillingModel': JointIntentAndSlotFillingModel})\n",
    "\n",
    "print(nlu(\"could you please change my reservation on december twenty first originally scheduled for twelve thirty in the morning to now be at four thirty five pm\", tokenizer, joint_model, intent_names, slot_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
