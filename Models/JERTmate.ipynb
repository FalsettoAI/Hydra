{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Bert Model for slot and intent classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, CategoricalAccuracy\n",
    "\n",
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "intentOutputs = []\n",
    "slotOutputs = []\n",
    "\n",
    "with open(\"../processing/JERTmate_final_data.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "    inputs = data[\"inputs\"]\n",
    "    intentOutputs = data[\"intentOutputs\"]\n",
    "    slotOutputs = data[\"slotOutputs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data - Train 80% | Validation 10% | Test 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10324, 330)\n",
      "(10324, 38)\n"
     ]
    }
   ],
   "source": [
    "def split_arrays(inputs, intentOutputs, slotOutputs, train_ratio, val_ratio, test_ratio, seed=42):\n",
    "    assert len(inputs) == len(intentOutputs) == len(slotOutputs), \"All arrays must have the same length\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(inputs))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    n_total = len(inputs)\n",
    "    n_train = int(n_total * train_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    \n",
    "    inputs_train, inputs_val, inputs_test = inputs[:n_train], inputs[n_train:n_train + n_val], inputs[n_train + n_val:]\n",
    "    intentOutputs_train, intentOutputs_val, intentOutputs_test = intentOutputs[:n_train], intentOutputs[n_train:n_train + n_val], intentOutputs[n_train + n_val:]\n",
    "    slotOutputs_train, slotOutputs_val, slotOutputs_test = slotOutputs[:n_train], slotOutputs[n_train:n_train + n_val], slotOutputs[n_train + n_val:]\n",
    "    \n",
    "    return (tf.constant(inputs_train), tf.constant(inputs_val), tf.constant(inputs_test)), (tf.constant(intentOutputs_train), tf.constant(intentOutputs_val), tf.constant(intentOutputs_test)), (tf.constant(slotOutputs_train), tf.constant(slotOutputs_val), tf.constant(slotOutputs_test))\n",
    "\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "(inputs_train, inputs_val, inputs_test), (intentOutputs_train, intentOutputs_val, intentOutputs_test), (slotOutputs_train, slotOutputs_val, slotOutputs_test) = split_arrays(inputs, intentOutputs, slotOutputs, train_ratio, val_ratio, test_ratio)\n",
    "\n",
    "print(slotOutputs_train.shape)\n",
    "print(intentOutputs_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "class JointIntentAndSlotFillingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, intent_vector_length=None, slot_vector_length=None, model_name=model_name, dropout_prob=0.08):\n",
    "        super().__init__(name=\"joint_intent_slot\")\n",
    "        #   ** GENERAL LAYERS **\n",
    "        self.bert = TFBertModel.from_pretrained(model_name) # BERT model\n",
    "        self.dropout = Dropout(dropout_prob) # basic dropout layer\n",
    "\n",
    "\n",
    "\n",
    "        #   ** SLOT LAYERS **\n",
    "        # LHS compressor\n",
    "        self.conv_layer = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 768), strides=(1, 1),padding='valid',activation='relu')\n",
    "        #self.sequence_output_processor = Dense(150, activation=\"relu\", name=\"sequence_output_processor\")\n",
    "\n",
    "        # processing layers\n",
    "        self.slot_processor = Dense(((150) + (intent_vector_length * 2) + (slot_vector_length * 2)), activation=\"relu\", name=\"slot_processor\")\n",
    "\n",
    "        # output layer\n",
    "        self.slot_output = Dense(slot_vector_length, activation='relu', name=\"slot_output\")\n",
    "\n",
    "\n",
    "\n",
    "        #  ** INTENT LAYERS **\n",
    "        # processing layers\n",
    "        self.intent_processor = Dense(((150 * 1) + (intent_vector_length * 2)), activation=\"relu\", name=\"intent_processor\")\n",
    "\n",
    "        # output layer\n",
    "        self.intent_output = Dense(intent_vector_length, activation='softmax', name=\"intent_output\")\n",
    "\n",
    "    def __call__(self, inputs, **kwargs):\n",
    "        # run BERT\n",
    "        trained_bert = self.bert(inputs[:, :150], **kwargs)\n",
    "        pooled_output = trained_bert.pooler_output\n",
    "        sequence_output = trained_bert.last_hidden_state\n",
    "\n",
    "        #   ** SLOT CLASSIFICATION **\n",
    "        # reduce dimensionality of sequence output - Mini CNN - [batch_size, sequence_length, 1]\n",
    "        conv_output = self.conv_layer(tf.expand_dims(sequence_output, axis=-1))\n",
    "\n",
    "        # flatten sequence output\n",
    "        compressed_sequence_output = tf.squeeze(conv_output, axis=[-1, -2])\n",
    "\n",
    "        # slot processor\n",
    "        slot_processor_input = self.dropout(tf.concat([compressed_sequence_output, tf.cast(inputs[:, 150:], dtype=tf.float32)], axis=-1), training=kwargs.get(\"training\", False))\n",
    "        slot_processor_output = self.slot_processor(slot_processor_input)\n",
    "\n",
    "        # slot output\n",
    "        slot_output_input = self.dropout(slot_processor_output, training=kwargs.get(\"training\", False))\n",
    "        slot_logits = self.slot_output(slot_output_input)\n",
    "        print(slot_logits.shape)\n",
    "\n",
    "\n",
    "\n",
    "        #   ** INTENT CLASSIFICATION **\n",
    "        # intent processor\n",
    "        intent_processor_input = self.dropout(tf.concat([pooled_output, tf.cast(inputs[:, 150:150 + 114], dtype=tf.float32)], axis=-1), training=kwargs.get(\"training\", False))\n",
    "        intent_processor_output = self.intent_processor(intent_processor_input)\n",
    "\n",
    "        # intent output\n",
    "        intent_output_input = self.dropout(intent_processor_output, training=kwargs.get(\"training\", False))\n",
    "        intent_logits = self.intent_output(intent_output_input)\n",
    "        print(intent_logits.shape)\n",
    "\n",
    "        return slot_logits, intent_logits\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(JointIntentAndSlotFillingModel, self).get_config()\n",
    "        config.update({\n",
    "            \"dropout\": self.dropout_prob,\n",
    "            \"intent_vector_length\": self.intent_vector_length,\n",
    "            \"slot_vector_length\": self.slot_vector_length,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "joint_model = JointIntentAndSlotFillingModel(intent_vector_length=38, slot_vector_length=330)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "(None, 330)\n",
      "(None, 38)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 330), output.shape=(None, 330)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# compile model\u001b[39;00m\n\u001b[0;32m     24\u001b[0m joint_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mopt, loss\u001b[38;5;241m=\u001b[39mlosses, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[1;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mslotOutputs_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintentOutputs_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mslotOutputs_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintentOutputs_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlearning_rate_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\falkt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:619\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    614\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    615\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    616\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    617\u001b[0m     )\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m--> 619\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must have rank (ndim) `target.ndim - 1`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    622\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    623\u001b[0m     )\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[1;31mValueError\u001b[0m: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 330), output.shape=(None, 330)"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "opt = Adam(learning_rate=5e-5, epsilon=1e-08)\n",
    "\n",
    "# learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 7:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.97\n",
    "learning_rate_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=\"model_epoch_{epoch:02d}.keras\",  # Save the model with the epoch number in the filename\n",
    "    save_freq='epoch',\n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "# two losses, one for slots, another for intents\n",
    "losses = [SparseCategoricalCrossentropy(from_logits=True, name=\"slot_loss\"), CategoricalCrossentropy(from_logits=True, name=\"intent_loss\")]\n",
    "metrics = [SparseCategoricalAccuracy(name=\"slot_accuracy\"), CategoricalAccuracy(name=\"intent_accuracy\")]\n",
    "\n",
    "# compile model\n",
    "joint_model.compile(optimizer=opt, loss=losses, metrics=metrics)\n",
    "\n",
    "history = joint_model.fit(\n",
    "    x=inputs_train, \n",
    "    y=(slotOutputs_train, intentOutputs_train), \n",
    "    validation_data=(inputs_val, (slotOutputs_val, intentOutputs_val)), \n",
    "    epochs=10, \n",
    "    batch_size=16,\n",
    "    shuffle=True, \n",
    "    callbacks=[learning_rate_callback, checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "\n",
    "keep in mind -> test slot accuracy will be higher than reality.\n",
    "        Because 90% of the data points are 0, it can just guess 0 and be right 85% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_slot_acc, test_intent_acc = joint_model.evaluate(x=inputs_test, y=(slotOutputs_test, intentOutputs_test), batch_size=32)\n",
    "\n",
    "print(f\"Test Slot Accuracy: {test_slot_acc}\")\n",
    "print(f\"Test Intent Accuracy: {test_intent_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlu(text, tokenizer, model, intent_names, slot_names):\n",
    "    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n",
    "    outputs = model(inputs)\n",
    "    slot_logits, intent_logits = outputs\n",
    "\n",
    "    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, :]\n",
    "    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n",
    "\n",
    "    info = {\"intent\": intent_names[intent_id], \"slots\": {}}\n",
    "\n",
    "    out_dict = {}\n",
    "    # get all slot names and add to out_dict as keys\n",
    "    predicted_slots = set([re.sub(r'^[BI]-', '', slot_names[s]) for s in slot_ids if s != 0])\n",
    "    for ps in predicted_slots:\n",
    "      out_dict[ps] = []\n",
    "\n",
    "    # check if the text starts with a small letter\n",
    "    if text[0].islower():\n",
    "      tokens = tokenizer.tokenize(text, add_special_tokens=True)\n",
    "    else:\n",
    "      tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # process sequence output for slots\n",
    "    for token, slot_id in zip(tokens, slot_ids):\n",
    "      # add all to out_dict\n",
    "      slot_name = slot_names[slot_id]\n",
    "\n",
    "      if slot_name == \"[PAD]\":\n",
    "        continue\n",
    "\n",
    "      # collect tokens\n",
    "      collected_tokens = [token]\n",
    "      idx = tokens.index(token)\n",
    "\n",
    "      # see if it starts with ##\n",
    "      # then it belongs to the previous token\n",
    "      if token.startswith(\"##\"):\n",
    "\n",
    "        # check if the token already exists or not\n",
    "        if tokens[idx - 1] not in out_dict[slot_name]:\n",
    "          collected_tokens.insert(0, tokens[idx - 1])\n",
    "\n",
    "      # add collected tokens to slots\n",
    "      out_dict[slot_name].extend(collected_tokens)\n",
    "\n",
    "    # process out_dict\n",
    "    for slot_name in out_dict:\n",
    "      tokens = out_dict[slot_name]\n",
    "      slot_value = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "      info[\"slots\"][slot_name] = slot_value.strip()\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = tf.keras.models.load_model('model_epoch_18.keras', custom_objects={'JointIntentAndSlotFillingModel': JointIntentAndSlotFillingModel})\n",
    "\n",
    "print(nlu(\"could you please change my reservation on december twenty first originally scheduled for twelve thirty in the morning to now be at four thirty five pm\", tokenizer, joint_model, intent_names, slot_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
